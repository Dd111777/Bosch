# -*- coding: utf-8 -*-

import os
import copy
import math
import json
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from typing import Dict, List, Tuple, Optional

from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from torch.optim.lr_scheduler import LambdaLR
from sklearn.model_selection import StratifiedKFold

# ====== ä¾èµ– util/æ¨¡å‹ï¼ˆä¿æŒä¸å˜ï¼‰ ======
from physio_util import (
    set_seed, export_predictions_longtable, export_metrics_grid,
    write_summary_txt, heatmap, parity_scatter, residual_hist, save_manifest,
    metrics, transform_for_display, FAMILIES,
    excel_to_physics_dataset,
    load_new_excel_as_sparse_morph, build_sparse_batch
)
from phys_model import TemporalRegressor, PhysicsSeqPredictor


# ========================== é…ç½®ï¼ˆ30æ ·æœ¬ä¼˜åŒ–ç‰ˆï¼‰ ==========================
class Cfg:
    # ==================== æ•°æ®è·¯å¾„ï¼ˆä¸å˜ï¼‰ ====================
    old_excel = r"D:\data\pycharm\bosch\case.xlsx"
    new_excel = r"D:\data\pycharm\bosch\Bosch.xlsx"
    save_root = "./runs_stageC_performance"  # æ€§èƒ½ä¼˜åŒ–ç‰ˆ

    # é¢„è®­ç»ƒæƒé‡
    phys_ckpt_F = "./runs_phys_split/F_Flux/phys_best.pth"
    phys_ckpt_I = "./runs_phys_split/Ion_Flux/phys_best.pth"
    morph_ckpt = "./runs_morph_old/morph_best_overall.pth"

    test_size = 0.10  # â¬‡ï¸ ä»0.15é™åˆ°0.10 (3æ ·æœ¬)
    val_size = 0.10  # â¬‡ï¸ ä»0.15é™åˆ°0.10 (3æ ·æœ¬)
    split_random_state = 42

    # ==================== è®­ç»ƒå‚æ•°ï¼ˆâ¬‡ï¸ ä¿å®ˆç­–ç•¥ï¼‰ ====================
    seed = 42
    seeds = [42, 43, 44]  # âœ… æ¢å¤3 seeds
    max_epochs = 3000  # â¬†ï¸ å¢åŠ åˆ°3000è½®

    # å­¦ä¹ ç‡ï¼ˆæ›´æ¿€è¿›ï¼‰
    lr_morph = 3e-4  # â¬†ï¸ ä»1e-4æé«˜åˆ°3e-4
    lr_phys = 1e-40  # âœ… ä¿æŒå†»ç»“
    lr_calib = 3e-4  # â¬†ï¸ ä»1e-4æé«˜åˆ°3e-4

    # æƒé‡è¡°å‡ï¼ˆé€‚åº¦æ­£åˆ™åŒ–ï¼‰
    wd_morph = 5e-3  # â¬†ï¸ ä»1e-3æé«˜åˆ°5e-3
    wd_phys = 1e-2  # âœ… ä¿æŒ
    wd_calib = 1e-5  # â¬‡ï¸ ä»1e-4é™åˆ°1e-5

    # æ¢¯åº¦è£å‰ª
    batch_clip = 1.0  # â¬†ï¸ ä»0.5æ¢å¤åˆ°1.0

    # Batch size
    batch_size = 8
    use_full_batch = True

    dropout_morph = 0.3  # â¬‡ï¸ ä»0.5é™åˆ°0.3
    dropout_calib = 0.05  # â¬‡ï¸ ä»0.1é™åˆ°0.05

    loss_delta = 1.0
    loss_smooth_weight = 5e-3  # â¬†ï¸ ä»1e-3æé«˜åˆ°5e-3
    mono_zmin_weight = 3e-3  # â¬†ï¸ ä»1e-3æé«˜åˆ°3e-3

    # ==================== Ionç›¸å…³ ====================
    ion_affine_default = {"a": 1.0, "b": 0.0, "c": 0.0}
    ion_learnable_lr = 5e-5  # â¬†ï¸ ä»1e-6æ¢å¤åˆ°5e-5
    ion_learnable_wd = 1e-6  # â¬‡ï¸ ä»1e-5æ¢å¤åˆ°1e-6

    # ==================== å±•ç¤ºç©ºé—´ï¼ˆä¸å˜ï¼‰ ====================
    unit_scale = 1
    flip_sign = False
    clip_nonneg = False
    min_display_value = 0.0
    family_sign = np.array([-1, +1, +1, +1, +1, +1], dtype=np.float32)
    sheet_name = "case"
    HEIGHT_FAMILY = "h1"

    use_smape = True
    mape_eps_nm = 0.001

    use_ema = True
    ema_decay = 0.995  # â¬†ï¸ ä»0.99æé«˜åˆ°0.995

    early_stop = True
    early_stop_patience = 100  # â¬†ï¸ ä»50æé«˜åˆ°100

    use_scheduler = True
    scheduler_type = "cosine_warmup"
    warmup_epochs = 50  # â¬†ï¸ ä»20å¢åŠ åˆ°50
    min_lr_ratio = 0.001  # â¬‡ï¸ ä»0.01é™åˆ°0.001

    use_augmentation = True
    aug_noise_std = 0.005  # â¬‡ï¸ ä»0.01é™åˆ°0.005
    aug_time_jitter = False

    # ==================== åæ ¡å‡† ====================
    calib_min_points = 5  # â¬‡ï¸ ä»12é™åˆ°5ï¼ˆæ ·æœ¬å°‘ï¼‰
    calib_ridge = 1e-4  # Ridgeæ­£åˆ™åŒ–

    # ==================== æ¨¡å‹å†»ç»“ç­–ç•¥ï¼ˆâœ… å…³é”®æ–°å¢ï¼‰ ====================
    # è¿™äº›å‚æ•°ä¼šåœ¨å˜ä½“ä¸­è¦†ç›–
    default_freeze_phys = True  # é»˜è®¤å†»ç»“ç‰©ç†ç½‘
    default_freeze_morph_encoder = True  # âœ… é»˜è®¤å†»ç»“å½¢è²Œencoder
    default_freeze_morph_heads = False  # é»˜è®¤å¾®è°ƒå½¢è²Œheads

    variants = [
        # ===== åŸºç¡€ç»„ =====
        dict(
            name="baseline_calib_only",
            description="Only calibration head",
            use_adapter=False, use_derived=False, ion_gate="use",
            per_family_head=False, hetero=False, task_uncertainty=False,
            calib_head="affine_per_channel", post_calib="per_k",
            learnable_ion=False,
            freeze_phys=True, freeze_morph_encoder=True, freeze_morph_heads=True,
            stagewise_unfreeze=False,
            expected_trainable_params=100,
        ),
        dict(
            name="baseline_with_heads",
            description="Calibration + morph heads",
            use_adapter=False, use_derived=False, ion_gate="use",
            per_family_head=False, hetero=False, task_uncertainty=False,
            calib_head="affine_per_channel", post_calib="per_k",
            learnable_ion=False,
            freeze_phys=True, freeze_morph_encoder=True, freeze_morph_heads=False,
            stagewise_unfreeze=False,
            expected_trainable_params=1500,
        ),

        # ===== Adapterç³»åˆ— =====
        dict(
            name="adapter_light",
            description="Lightweight adapter",
            use_adapter=True, use_derived=False, ion_gate="use",
            per_family_head=False, hetero=False, task_uncertainty=False,
            calib_head="affine_per_channel", post_calib="per_k",
            learnable_ion=False,
            freeze_phys=True, freeze_morph_encoder=True, freeze_morph_heads=False,
            stagewise_unfreeze=False,
            expected_trainable_params=2000,
        ),
        dict(
            name="adapter_derived",
            description="Adapter + derived features",
            use_adapter=True, use_derived=True, ion_gate="use",
            per_family_head=False, hetero=False, task_uncertainty=False,
            calib_head="affine_per_channel", post_calib="per_k",
            learnable_ion=True,
            freeze_phys=True, freeze_morph_encoder=True, freeze_morph_heads=False,
            stagewise_unfreeze=False,
            expected_trainable_params=3000,
        ),

        # ===== Per-Family Headç³»åˆ— =====
        dict(
            name="adapter_head",
            description="Adapter + per-family head",
            use_adapter=True, use_derived=True, ion_gate="use",
            per_family_head=True, hetero=False, task_uncertainty=False,
            calib_head="affine_per_channel", post_calib="per_k",
            learnable_ion=True,
            freeze_phys=True, freeze_morph_encoder=True, freeze_morph_heads=False,
            stagewise_unfreeze=False,
            expected_trainable_params=5000,
        ),
        dict(
            name="adapter_head_hetero",
            description="Adapter + per-family head + heteroscedastic",
            use_adapter=True, use_derived=True, ion_gate="use",
            per_family_head=True, hetero=True, task_uncertainty=False,
            calib_head="affine_per_channel", post_calib="per_k",
            learnable_ion=True,
            freeze_phys=True, freeze_morph_encoder=True, freeze_morph_heads=False,
            stagewise_unfreeze=False,
            expected_trainable_params=6000,
        ),
        dict(
            name="adapter_head_hetero_taskuncert",
            description="Full bells and whistles",
            use_adapter=True, use_derived=True, ion_gate="use",
            per_family_head=True, hetero=True, task_uncertainty=True,
            calib_head="affine_per_channel", post_calib="per_k",
            learnable_ion=True,
            freeze_phys=True, freeze_morph_encoder=True, freeze_morph_heads=False,
            stagewise_unfreeze=False,
            expected_trainable_params=7000,
        ),

        # ===== Ion Gateç³»åˆ— =====
        dict(
            name="adapter_head_iongate",
            description="Ion gating mechanism",
            use_adapter=True, use_derived=True, ion_gate="gate",
            per_family_head=True, hetero=False, task_uncertainty=False,
            calib_head="affine_per_channel", post_calib="per_k",
            learnable_ion=True,
            freeze_phys=True, freeze_morph_encoder=True, freeze_morph_heads=False,
            stagewise_unfreeze=False,
            expected_trainable_params=6000,
        ),
        dict(
            name="adapter_head_iongate_hetero",
            description="Ion gating + heteroscedastic + task uncertainty",
            use_adapter=True, use_derived=True, ion_gate="gate",
            per_family_head=True, hetero=True, task_uncertainty=True,
            calib_head="affine_per_channel", post_calib="per_k",
            learnable_ion=True,
            freeze_phys=True, freeze_morph_encoder=True, freeze_morph_heads=False,
            stagewise_unfreeze=False,
            expected_trainable_params=7500,
        ),

        # ===== åæ ¡å‡†ç­–ç•¥å¯¹æ¯” =====
        dict(
            name="adapter_head_postcalib_kt",
            description="Per-kt posterior calibration",
            use_adapter=True, use_derived=True, ion_gate="use",
            per_family_head=True, hetero=False, task_uncertainty=False,
            calib_head="affine_per_channel", post_calib="per_kt",
            learnable_ion=True,
            freeze_phys=True, freeze_morph_encoder=True, freeze_morph_heads=False,
            stagewise_unfreeze=False,
            expected_trainable_params=5000,
        ),

        # ===== è§£å†»encoderï¼ˆæ¿€è¿›æ–¹æ¡ˆï¼‰ =====
        dict(
            name="adapter_head_encoder",
            description="Unfreeze morph encoder (risky)",
            use_adapter=True, use_derived=True, ion_gate="use",
            per_family_head=True, hetero=False, task_uncertainty=False,
            calib_head="affine_per_channel", post_calib="per_k",
            learnable_ion=True,
            freeze_phys=True, freeze_morph_encoder=False,  # è§£å†»ï¼
            freeze_morph_heads=False,
            stagewise_unfreeze=False,
            expected_trainable_params=20000,
        ),

        # ===== æ¸è¿›å¼è§£å†»ï¼ˆæ–°å¢ï¼‰ =====
        dict(
            name="adapter_head_stagewise",
            description="Stagewise unfreezing strategy",
            use_adapter=True, use_derived=True, ion_gate="use",
            per_family_head=True, hetero=False, task_uncertainty=False,
            calib_head="affine_per_channel", post_calib="per_k",
            learnable_ion=True,
            freeze_phys=True, freeze_morph_encoder=True,
            freeze_morph_heads=False,
            stagewise_unfreeze=True,  # å¯ç”¨æ¸è¿›è§£å†»
            expected_trainable_params=5000,
        ),
    ]

    # ==================== å…¶ä»–é«˜çº§é€‰é¡¹ï¼ˆå¯é€‰ï¼‰ ====================
    # Ioné—¨æ§ç­–ç•¥ablation
    ion_gate_variants = ["use", "zero", "const", "smooth", "gate"]

    # æ˜¯å¦è¿›è¡Œå¿«é€Ÿè¯Šæ–­æ£€æŸ¥
    quick_check = False

    # æ—¥å¿—å’Œä¿å­˜
    save_interval = 50  # æ¯50è½®ä¿å­˜ä¸€æ¬¡
    log_interval = 10  # æ¯10è½®æ‰“å°ä¸€æ¬¡
    save_best_only = True  # åªä¿å­˜æœ€ä½³æ¨¡å‹

    # å¯è§†åŒ–
    plot_diagnostics = True  # ç”Ÿæˆé€familyè¯Šæ–­å›¾
    plot_temporal_error = True  # æ—¶åºè¯¯å·®åˆ†æ

# ========================== è¾…åŠ©å‡½æ•° ==========================
def print_config_summary():
    """æ‰“å°é…ç½®æ‘˜è¦"""
    print("=" * 80)
    print("StageC Configuration Summary (Performance Optimized)")
    print("=" * 80)

    print("Data Split:")
    print(f"  - Train: {100 - (Cfg.test_size + Cfg.val_size) * 100:.0f}%")
    print(f"  - Val:   {Cfg.val_size * 100:.0f}%")
    print(f"  - Test:  {Cfg.test_size * 100:.0f}%")

    print("\nTraining:")
    print(f"  - Max Epochs: {Cfg.max_epochs}")
    print(f"  - Seeds: {Cfg.seeds}")
    print(f"  - LR Morph: {Cfg.lr_morph}")
    print(f"  - WD Morph: {Cfg.wd_morph}")
    print(f"  - Dropout: {Cfg.dropout_morph}")

    print("\nRegularization:")
    print(f"  - EMA Decay: {Cfg.ema_decay}")
    print(f"  - Early Stop Patience: {Cfg.early_stop_patience}")
    print(f"  - Data Augmentation: {Cfg.use_augmentation}")

    print(f"\nVariants: {len(Cfg.variants)}")
    for i, v in enumerate(Cfg.variants, 1):
        print(f"  {i}. {v['name']}: {v.get('description', 'N/A')}")
        # å®‰å…¨è·å–å¯é€‰å­—æ®µ
        r2_range = v.get('expected_r2_range', 'N/A')
        params = v.get('expected_trainable_params', 'N/A')
        if isinstance(params, int):
            print(f"     Expected RÂ²: {r2_range}, Params: ~{params:,}")
        else:
            print(f"     Expected RÂ²: {r2_range}, Params: {params}")

    print("=" * 80)
    print()

def ensure_dir(path: str):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    os.makedirs(path, exist_ok=True)

def _norm_col(x, mean, std):
    """åˆ—å½’ä¸€åŒ–"""
    # è½¬æ¢Tensorä¸ºnumpy
    if isinstance(mean, torch.Tensor):
        mean = mean.cpu().numpy()
    if isinstance(std, torch.Tensor):
        std = std.cpu().numpy()

    return (x - mean) / (std + 1e-8)


def load_fallback_sparse(merged_excel_path, norm_mean, norm_std, time_values, families):
    if merged_excel_path.endswith('.csv'):
        df = pd.read_csv(merged_excel_path)
    else:
        df = pd.read_excel(merged_excel_path)

    # === ä»¥ä¸‹é€»è¾‘100%ä¿ç•™ ===
    s8_cols = [c for c in df.columns if str(c).lower().startswith("s") and str(c)[1:].isdigit()]
    s8_cols = sorted(s8_cols, key=lambda s: int(str(s)[1:]))[:7]
    s8_raw = df[s8_cols].astype(float).values
    s8 = _norm_col(s8_raw, norm_mean, norm_std).astype(np.float32)

    fams = list(families)
    if "zmin" not in fams:
        fams = ["zmin"] + fams
    T = len(time_values)
    fam2idx = {n: i for i, n in enumerate(fams)}
    B, K = s8.shape[0], len(fams)

    y_sparse = torch.zeros((B, K, T), dtype=torch.float32)
    m_sparse = torch.zeros((B, K, T), dtype=torch.bool)
    tv = np.array(time_values, dtype=float).tolist()

    def t_idx(tnum):
        return int(tv.index(float(tnum))) if float(tnum) in tv else max(0, min(T - 1, int(tnum) - 1))

    def _nm_to_um(vals):
        return np.nan_to_num(vals.astype(float), nan=np.nan) / 1000.0

    def _pick(df, cands):
        for c in cands:
            if c in df.columns:
                return c
        return None

    def _fill(k_idx, cands, times, negate=False):
        for n in times:
            col = _pick(df, cands(n))
            if col is None:
                continue
            vals = _nm_to_um(df[col].values)
            if negate:
                vals = -vals
            ti = t_idx(n)
            v_t = torch.tensor(vals, dtype=torch.float32)
            ok = torch.isfinite(v_t)
            if ok.any():
                y_sparse[:, k_idx, ti][ok] = v_t[ok]
                m_sparse[:, k_idx, ti][ok] = True

    # zmin
    if "zmin" in fams and "zmin_10" in df.columns:
        k = fam2idx["zmin"]
        vals = _nm_to_um(df["zmin_10"].values) * (-1.0)
        ti = t_idx(10)
        v_t = torch.tensor(vals, dtype=torch.float32)
        ok = torch.isfinite(v_t)
        if ok.any():
            y_sparse[:, k, ti][ok] = v_t[ok]
            m_sparse[:, k, ti][ok] = True

    # h1, d1, w
    def _cands_h(n):
        return [f"h1_{n}", f"h1{n}", f"h{n}", f"h_{n}", f"{n}thscallopheight"]

    def _cands_d(n):
        return [f"d1_{n}", f"d1{n}", f"d{n}", f"d_{n}", f"{n}thscallopdepth"]

    def _cands_w(n):
        return [f"w{n}", f"w_{n}", f"W{n}", f"W {n}", f"{n}thscallopwidth"]

    if "h1" in fams:
        _fill(fam2idx["h1"], _cands_h, [3, 5, 9], negate=False)
    if "d1" in fams:
        _fill(fam2idx["d1"], _cands_d, [3, 5, 9], negate=False)
    if "w" in fams:
        _fill(fam2idx["w"], _cands_w, [1, 3, 5, 9], negate=False)

    s8 = torch.tensor(s8, dtype=torch.float32)
    tvals = torch.tensor(np.array(time_values, dtype=np.float32), dtype=torch.float32)

    # è¿”å›sample_idsç”¨äºåˆ’åˆ†
    sample_ids = np.arange(B)
    return s8, y_sparse, m_sparse, tvals, fams, sample_ids


def load_data_with_split(new_excel, norm_mean, norm_std, time_values, families,
                         test_size, val_size, seed):
    # 1. åŠ è½½æ•°æ®ï¼ˆå¯¹é½é€»è¾‘ä¿æŒä¸å˜ï¼‰
    try:
        from physio_util import load_new_excel_as_sparse_morph, build_sparse_batch
        recs = load_new_excel_as_sparse_morph(new_excel, height_family=Cfg.HEIGHT_FAMILY)
        s8, y_sparse, m_sparse, tvals = build_sparse_batch(recs, norm_mean, norm_std, time_values)
        fams = families
        sample_ids = np.arange(s8.shape[0])
        print("[DataSplit] Loaded via util loader.")
    except Exception as e:
        print(f"[DataSplit] Fallback loader: {e}")
        s8, y_sparse, m_sparse, tvals, fams, sample_ids = load_fallback_sparse(
            new_excel, norm_mean, norm_std, time_values, families
        )

    # 2. æ•°æ®åˆ’åˆ†ï¼ˆæ–°å¢ï¼‰
    B = s8.shape[0]

    # åˆ†å±‚ä¾æ®ï¼šæ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆç‚¹æ•°
    sample_counts = m_sparse.sum(dim=(1, 2)).numpy()
    strata = (sample_counts > np.median(sample_counts)).astype(int)

    # Train/Temp split
    train_idx, temp_idx = train_test_split(
        sample_ids, test_size=(test_size + val_size),
        random_state=seed, stratify=strata
    )

    # Val/Test split
    temp_strata = strata[temp_idx]
    val_idx, test_idx = train_test_split(
        temp_idx, test_size=(test_size / (test_size + val_size)),
        random_state=seed, stratify=temp_strata
    )

    print(f"[DataSplit] Train={len(train_idx)}, Val={len(val_idx)}, Test={len(test_idx)}")

    # 3. é‡æ–°è®¡ç®—ç»Ÿè®¡é‡ï¼ˆä»…ç”¨è®­ç»ƒé›†ï¼‰
    s8_train = s8[train_idx]
    # åå½’ä¸€åŒ–åˆ°åŸå§‹åŸŸ
    s8_train_raw = s8_train * (norm_std + 1e-8) + norm_mean
    train_mean = s8_train_raw.mean(dim=0)
    train_std = s8_train_raw.std(dim=0).clamp_min(1e-8)

    # å…¨é‡æ•°æ®ç”¨è®­ç»ƒé›†ç»Ÿè®¡é‡é‡æ–°æ ‡å‡†åŒ–
    s8_full_raw = s8 * (norm_std + 1e-8) + norm_mean
    s8_renorm = (s8_full_raw - train_mean) / train_std

    return {
        "s8": s8_renorm,
        "s8_raw": s8_full_raw,
        "orig_norm_mean": norm_mean,
        "orig_norm_std": norm_std,

        "y_sparse": y_sparse,
        "m_sparse": m_sparse,
        "tvals": tvals,
        "families": fams,
        "splits": {
            "train": train_idx,
            "val": val_idx,
            "test": test_idx
        },
        "train_stats": {
            "mean": train_mean,
            "std": train_std
        }
    }


# ========================== æ•°æ®ç»Ÿè®¡å’Œæ£€æŸ¥ ==========================
def print_data_statistics(data_dict):
    """æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    s8 = data_dict["s8"]
    y_sparse = data_dict["y_sparse"]
    m_sparse = data_dict["m_sparse"]
    fams = data_dict["families"]
    splits = data_dict["splits"]

    print("\n" + "=" * 80)
    print("Data Statistics")
    print("=" * 80)

    # æ ·æœ¬æ•°
    B, K, T = y_sparse.shape
    print(f"Total samples: {B}")
    print(f"Families: {K} ({', '.join(fams)})")
    print(f"Time steps: {T}")

    # åˆ’åˆ†
    print(f"\nData Split:")
    print(f"  Train: {len(splits['train'])} samples ({len(splits['train']) / B * 100:.1f}%)")
    print(f"  Val:   {len(splits['val'])} samples ({len(splits['val']) / B * 100:.1f}%)")
    print(f"  Test:  {len(splits['test'])} samples ({len(splits['test']) / B * 100:.1f}%)")

    # é€Familyæœ‰æ•ˆç‚¹æ•°
    print(f"\nValid points per family:")
    for k, fam in enumerate(fams):
        n_valid = m_sparse[:, k, :].sum().item()
        pct = n_valid / (B * T) * 100
        print(f"  {fam:8s}: {int(n_valid):4d} / {B * T:4d} ({pct:5.1f}%)")

    # é€splitæœ‰æ•ˆç‚¹æ•°
    print(f"\nValid points per split:")
    for split_name, idx in splits.items():
        n_valid = m_sparse[idx].sum().item()
        total = len(idx) * K * T
        pct = n_valid / total * 100 if total > 0 else 0
        print(f"  {split_name:5s}: {int(n_valid):5d} / {total:5d} ({pct:5.1f}%)")

    # é™æ€ç‰¹å¾ç»Ÿè®¡
    print(f"\nStatic features (s8) - train set:")
    s8_train = s8[splits["train"]]
    print(f"  Mean: {s8_train.mean(dim=0).numpy()}")
    print(f"  Std:  {s8_train.std(dim=0).numpy()}")

    print("=" * 80 + "\n")


def check_data_quality(data_dict):
    """æ£€æŸ¥æ•°æ®è´¨é‡å¹¶ç»™å‡ºè­¦å‘Š"""
    s8 = data_dict["s8"]
    y_sparse = data_dict["y_sparse"]
    m_sparse = data_dict["m_sparse"]
    splits = data_dict["splits"]

    warnings = []

    # æ£€æŸ¥1: è®­ç»ƒé›†æ ·æœ¬æ•°
    n_train = len(splits["train"])
    if n_train < 20:
        warnings.append(f"âš ï¸ Train samples ({n_train}) < 20, high overfitting risk!")
    elif n_train < 30:
        warnings.append(f"âš ï¸ Train samples ({n_train}) < 30, consider increasing")

    # æ£€æŸ¥2: éªŒè¯/æµ‹è¯•é›†æ ·æœ¬æ•°
    n_val = len(splits["val"])
    n_test = len(splits["test"])
    if n_val < 3 or n_test < 3:
        warnings.append(f"âš ï¸ Val ({n_val}) or Test ({n_test}) < 3, unreliable evaluation")

    # æ£€æŸ¥3: æœ‰æ•ˆç‚¹åˆ†å¸ƒ
    for split_name, idx in splits.items():
        if len(idx) == 0:
            continue
        m_split = m_sparse[idx]
        valid_per_sample = m_split.sum(dim=(1, 2)).float().numpy()
        if valid_per_sample.min() < 3:
            warnings.append(f"âš ï¸ {split_name} set has samples with <3 valid points")

    # æ£€æŸ¥4: NaN/Inf
    if torch.isnan(s8).any() or torch.isinf(s8).any():
        warnings.append(f"âš ï¸ s8 contains NaN or Inf")
    if torch.isnan(y_sparse[m_sparse]).any():
        warnings.append(f"âš ï¸ y_sparse contains NaN in valid positions")

    # æ‰“å°è­¦å‘Š
    if warnings:
        print("\n" + "=" * 80)
        print("DATA QUALITY WARNINGS")
        print("=" * 80)
        for w in warnings:
            print(w)
        print("=" * 80 + "\n")
    else:
        print("[âœ“] Data quality check passed\n")

    return len(warnings) == 0


# ========================== ä¸»åŠ è½½æµç¨‹ ==========================
def load_all_data(meta_old):
    print("\n" + "=" * 80)
    print("Loading Data for Stage C (30-Sample Optimized)")
    print("=" * 80)

    # æå–æ—§è¡¨ç»Ÿè®¡é‡
    norm_mean = meta_old.get("norm_mean", torch.zeros(7))
    norm_std = meta_old.get("norm_std", torch.ones(7))
    time_values = meta_old.get("time_values", list(range(10)))
    families = meta_old.get("families", FAMILIES)

    if isinstance(norm_mean, dict):
        norm_mean = torch.tensor([norm_mean.get(f"s{i + 1}", 0.0) for i in range(7)])
    if isinstance(norm_std, dict):
        norm_std = torch.tensor([norm_std.get(f"s{i + 1}", 1.0) for i in range(7)])

    # åŠ è½½æ–°è¡¨æ•°æ®ï¼ˆå¸¦åˆ’åˆ†ï¼‰
    data_dict = load_data_with_split(
        Cfg.new_excel,
        norm_mean, norm_std,
        time_values, families,
        Cfg.test_size, Cfg.val_size,
        Cfg.split_random_state
    )

    # æ‰“å°ç»Ÿè®¡
    print_data_statistics(data_dict)

    # è´¨é‡æ£€æŸ¥
    check_data_quality(data_dict)

    return data_dict
# ========================== AUTO-SWEEP HELPERSï¼ˆæ–°å¢ï¼‰ ==========================
def _masked_data_only_family(data_dict: Dict, fam_index: int) -> Dict:
    """åªä¿ç•™ç¬¬ fam_index ä¸ª family çš„æœ‰æ•ˆæ©ç ï¼Œå…¶å®ƒ family æ¸…é›¶ï¼›ä¸æ”¹åŠ¨ y_sparse æœ¬ä½“ã€‚"""
    dd = copy.deepcopy(data_dict)
    m = dd["m_sparse"].clone()
    keep = torch.zeros_like(m)
    keep[:, fam_index:fam_index+1, :] = m[:, fam_index:fam_index+1, :]
    dd["m_sparse"] = keep
    return dd


def _decorate_variant_name(variant: Dict, suffix: str) -> Dict:
    """æµ…æ‹·è´ variantï¼Œå¹¶åœ¨ name åè¿½åŠ åç¼€ï¼Œé¿å…è¦†ç›–åŒåç»“æœç›®å½•ã€‚"""
    v = dict(variant)
    v["name"] = f"{variant['name']}_{suffix}"
    return v


class CalibAffinePerChannel(nn.Module):
    """é€é€šé“ä»¿å°„æ ¡å‡†"""

    def __init__(self, K, init_alpha=1.0, init_beta=0.0):
        super().__init__()
        self.alpha = nn.Parameter(torch.full((K,), float(init_alpha)))
        self.beta = nn.Parameter(torch.full((K,), float(init_beta)))

    def forward(self, y):
        return self.alpha.view(1, -1, 1) * y + self.beta.view(1, -1, 1)


class CalibTimeConv(nn.Module):
    """æ—¶é—´å·ç§¯æ ¡å‡†"""

    def __init__(self, K, kernel_size=3):
        super().__init__()
        padding = (kernel_size - 1) // 2
        self.dw = nn.Conv1d(K, K, kernel_size, padding=padding, groups=K, bias=True)
        with torch.no_grad():
            self.dw.weight.zero_()
            center = padding
            for k in range(K):
                self.dw.weight[k, 0, center] = 1.0
            self.dw.bias.zero_()

    def forward(self, y):
        return self.dw(y)


class CalibHybrid(nn.Module):
    """æ··åˆæ ¡å‡†"""

    def __init__(self, K, kernel_size=3):
        super().__init__()
        self.aff = CalibAffinePerChannel(K)
        self.tcv = CalibTimeConv(K, kernel_size=kernel_size)

    def forward(self, y):
        return self.tcv(self.aff(y))


def build_calib_head(kind: str, K: int) -> nn.Module:
    """æ„å»ºæ ¡å‡†å¤´"""
    if kind == "affine_per_channel":
        return CalibAffinePerChannel(K)
    elif kind == "time_conv":
        return CalibTimeConv(K)
    elif kind == "hybrid":
        return CalibHybrid(K)
    else:
        raise ValueError(f"Unknown calib_head: {kind}")


# ========================== æ¥å£å¢å¼ºæ¨¡å—ï¼ˆä¿ç•™åŸå§‹å®ç°ï¼‰ ==========================
class PhysAdapter(nn.Module):
    """æ·±åº¦å¯åˆ†ç¦»å·ç§¯é€‚é…å™¨"""

    def __init__(self, in_ch=2, k=3):
        super().__init__()
        pad = (k - 1) // 2
        self.dw = nn.Conv1d(in_ch, in_ch, k, padding=pad, groups=in_ch, bias=True)
        self.pw = nn.Conv1d(in_ch, in_ch, 1, bias=True)
        with torch.no_grad():
            self.dw.weight.zero_()
            self.dw.bias.zero_()
            for c in range(in_ch):
                self.dw.weight[c, 0, pad] = 1.0
            self.pw.weight.zero_()
            for c in range(in_ch):
                self.pw.weight[c, c, 0] = 1.0
            self.pw.bias.zero_()

    def forward(self, x):
        return self.pw(self.dw(x))


class PhysFeaReducer(nn.Module):
    """æ´¾ç”Ÿç‰¹å¾æå–å™¨"""

    def __init__(self, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.pw = nn.Conv1d(5, 2, 1, bias=True)
        nn.init.zeros_(self.pw.weight)
        with torch.no_grad():
            self.pw.weight[0, 0, 0] = 1.0
            self.pw.weight[1, 1, 0] = 1.0
        nn.init.zeros_(self.pw.bias)

    def forward(self, F, I):
        logI = torch.log(I.clamp_min(self.eps))
        dF = F[:, :, 1:] - F[:, :, :-1]
        dI = I[:, :, 1:] - I[:, :, :-1]
        dF = torch.nn.functional.pad(dF, (1, 0), mode="replicate")
        dI = torch.nn.functional.pad(dI, (1, 0), mode="replicate")
        x = torch.cat([F, I, logI, dF, dI], dim=1)
        return self.pw(x)


class IonGate(nn.Module):
    """Ioné—¨æ§æœºåˆ¶"""

    def __init__(self, k=5):
        super().__init__()
        self.k = k
        self.mlp = nn.Sequential(
            nn.Conv1d(2, 4, 1), nn.ReLU(inplace=True),
            nn.Conv1d(4, 1, 1), nn.Sigmoid()
        )

    def forward(self, F, I):
        pad = (self.k - 1) // 2
        w = torch.ones(1, 1, self.k, device=I.device) / self.k
        I_s = nn.functional.conv1d(nn.functional.pad(I, (pad, pad), mode="replicate"), w)
        gate = self.mlp(torch.cat([F, I], dim=1))
        return gate * I + (1.0 - gate) * I_s


class PerFamilyHead(nn.Module):
    """é€Familyè¾“å‡ºå¤´"""

    def __init__(self, K, k=3):
        super().__init__()
        pad = (k - 1) // 2
        self.dw = nn.Conv1d(K, K, k, padding=pad, groups=K, bias=True)
        self.alpha = nn.Parameter(torch.ones(K))
        self.beta = nn.Parameter(torch.zeros(K))
        with torch.no_grad():
            self.dw.weight.zero_()
            for c in range(K):
                self.dw.weight[c, 0, pad] = 1.0
            self.dw.bias.zero_()

    def forward(self, y):
        y = self.dw(y)
        return self.alpha.view(1, -1, 1) * y + self.beta.view(1, -1, 1)


class HeteroHead(nn.Module):
    """å¼‚æ–¹å·®è¾“å‡ºå¤´ï¼ˆé¢„æµ‹å‡å€¼å’Œæ–¹å·®ï¼‰"""

    def __init__(self, K, per_family=True):
        super().__init__()
        self.per_family = per_family
        self.mu_head = PerFamilyHead(K) if per_family else nn.Identity()
        self.logv_head = nn.Sequential(
            nn.Conv1d(K, K, 1, groups=1, bias=True),
        )
        nn.init.zeros_(self.logv_head[0].weight)
        nn.init.constant_(self.logv_head[0].bias, math.log(1.0))

    def forward(self, y):
        mu = self.mu_head(y) if self.per_family else y
        logv = self.logv_head(mu.detach())
        return mu, logv


# ========================== Ionåå˜æ¢ ==========================
class IonInverseTransform(nn.Module):
    """Ionåå˜æ¢"""

    def __init__(self, init_abc: Dict[str, float], learnable: bool = False):
        super().__init__()
        a, b, c = init_abc.get("a", 1.0), init_abc.get("b", 0.0), init_abc.get("c", 0.0)
        if learnable:
            self.a = nn.Parameter(torch.tensor(float(a)))
            self.b = nn.Parameter(torch.tensor(float(b)))
            self.c = nn.Parameter(torch.tensor(float(c)))
        else:
            self.register_buffer("a", torch.tensor(float(a)), persistent=False)
            self.register_buffer("b", torch.tensor(float(b)), persistent=False)
            self.register_buffer("c", torch.tensor(float(c)), persistent=False)
        self.learnable = learnable

    def forward(self, z):
        arg = torch.clamp(self.a * z + self.b, min=-40.0, max=40.0)
        y = torch.exp(arg) - self.c
        y = torch.clamp(y, min=0.0, max=1e6)
        return y

    def reg(self):
        if not self.learnable:
            return 0.0
        return ((self.a - 1.0) ** 2 + (self.b ** 2) + (self.c ** 2))


# ========================== æ¨¡å‹å†»ç»“ç­–ç•¥ï¼ˆâœ… å…³é”®æ–°å¢ï¼‰ ==========================
def freeze_model_parts(model: nn.Module, freeze_encoder=True, freeze_heads=False, verbose=True):
    # å…ˆè§£å†»æ‰€æœ‰
    for param in model.parameters():
        param.requires_grad = True

    frozen_params = []

    # å†»ç»“encoder
    if freeze_encoder:
        for name, param in model.named_parameters():
            if any(k in name for k in ['encoder', 'proj_in', 'pos']):
                param.requires_grad = False
                frozen_params.append(name)

    # å†»ç»“heads
    if freeze_heads:
        for name, param in model.named_parameters():
            if 'heads' in name:
                param.requires_grad = False
                frozen_params.append(name)

    # ç»Ÿè®¡å‚æ•°
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())

    if verbose:
        print(f"  [Freeze] Frozen {len(frozen_params)} parameter groups")
        if len(frozen_params) > 0 and len(frozen_params) < 10:
            for name in frozen_params:
                print(f"    - {name}")
        print(f"  [Params] Trainable: {trainable:,} / Total: {total:,} ({100 * trainable / total:.1f}%)")

    return trainable


def freeze_physics_models(phys_F, phys_I, verbose=True):
    for param in phys_F.parameters():
        param.requires_grad = False
    for param in phys_I.parameters():
        param.requires_grad = False

    if verbose:
        total_F = sum(p.numel() for p in phys_F.parameters())
        total_I = sum(p.numel() for p in phys_I.parameters())
        print(f"  [Freeze] Physics models frozen: F={total_F:,}, I={total_I:,}")


def print_trainable_params(model_dict: Dict[str, nn.Module]):
    print("\n" + "=" * 80)
    print("Trainable Parameters Summary")
    print("=" * 80)

    total_trainable = 0
    total_all = 0

    for name, model in model_dict.items():
        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total = sum(p.numel() for p in model.parameters())
        total_trainable += trainable
        total_all += total

        status = "âœ“ Training" if trainable > 0 else "âœ— Frozen"
        print(f"{name:20s}: {trainable:8,} / {total:8,} ({100 * trainable / max(total, 1):.1f}%) {status}")

    print("-" * 80)
    print(f"{'TOTAL':20s}: {total_trainable:8,} / {total_all:8,} ({100 * total_trainable / max(total_all, 1):.1f}%)")
    print("=" * 80 + "\n")


# ========================== EMA & Early Stopping ==========================
class EMA:
    def __init__(self, model: nn.Module, decay=0.99):
        self.decay = decay
        self.shadow = {}
        self.backup = None
        for n, p in model.named_parameters():
            if p.requires_grad:
                self.shadow[n] = p.data.clone()

    @torch.no_grad()
    def update(self, model: nn.Module):
        for n, p in model.named_parameters():
            if p.requires_grad and n in self.shadow:
                self.shadow[n].mul_(self.decay).add_(p.data, alpha=1.0 - self.decay)

    @torch.no_grad()
    def apply_to(self, model: nn.Module):
        # å¤‡ä»½ä¸€æ¬¡
        if self.backup is None:
            self.backup = {n: p.data.clone() for n, p in model.named_parameters() if p.requires_grad}
        for n, p in model.named_parameters():
            if p.requires_grad and n in self.shadow:
                p.data.copy_(self.shadow[n])

    @torch.no_grad()
    def restore(self, model: nn.Module):
        if self.backup is None:
            return
        for n, p in model.named_parameters():
            if p.requires_grad and n in self.backup:
                p.data.copy_(self.backup[n])
        self.backup = None

class EarlyStopper:
    """æ—©åœæœºåˆ¶"""

    def __init__(self, patience=50):
        self.best = None
        self.patience = patience
        self.count = 0

    def step(self, val):
        if self.best is None or val < self.best:
            self.best = val
            self.count = 0
            return True
        else:
            self.count += 1
            return self.count < self.patience

    def is_improved(self, val):
        """æ£€æŸ¥æ˜¯å¦æ”¹è¿›"""
        return self.best is None or val < self.best


# ========================== åæ ¡å‡†ï¼ˆPost-Calibrationï¼‰ ==========================
class CalibrationParams:
    """æ ¡å‡†å‚æ•°å®¹å™¨ï¼ˆå¯åºåˆ—åŒ–ï¼‰"""

    def __init__(self, method: str):
        self.method = method
        self.params = {}

    def to_dict(self):
        return {"method": self.method, "params": self.params}

    @classmethod
    def from_dict(cls, d):
        obj = cls(d["method"])
        obj.params = d["params"]
        return obj


def fit_calibration_params(y_pred_disp, y_true_disp, mask, method="per_k",
                           min_points=5, ridge=1e-4):
    yp = y_pred_disp.detach().cpu().numpy()
    yt = y_true_disp.detach().cpu().numpy()
    mk = mask.detach().cpu().numpy().astype(bool)
    B, K, T = yp.shape

    calib = CalibrationParams(method)

    if method == "per_k":
        for k in range(K):
            xs = yp[:, k, :][mk[:, k, :]]
            ys = yt[:, k, :][mk[:, k, :]]
            good = np.isfinite(xs) & np.isfinite(ys)
            xs, ys = xs[good], ys[good]

            if xs.size < min_points:
                calib.params[f"k{k}"] = {"a": 1.0, "b": 0.0}
                continue

            # çº¿æ€§å›å½’: y_true = a * y_pred + b
            X = np.column_stack([xs, np.ones_like(xs)])
            try:
                coef, *_ = np.linalg.lstsq(X, ys, rcond=None)
                a, b = float(coef[0]), float(coef[1])
            except Exception:
                XTX = X.T @ X + ridge * np.eye(2, dtype=X.dtype)
                XTy = X.T @ ys
                coef = np.linalg.solve(XTX, XTy)
                a, b = float(coef[0]), float(coef[1])

            calib.params[f"k{k}"] = {"a": a, "b": b}

    elif method == "per_kt":
        for k in range(K):
            for t in range(T):
                m = mk[:, k, t]
                xs = yp[m, k, t]
                ys = yt[m, k, t]
                good = np.isfinite(xs) & np.isfinite(ys)
                xs, ys = xs[good], ys[good]

                if xs.size < max(3, 2):  # é™ä½åˆ°3
                    calib.params[f"k{k}_t{t}"] = {"a": 1.0, "b": 0.0}
                    continue

                X = np.column_stack([xs, np.ones_like(xs)])
                try:
                    coef, *_ = np.linalg.lstsq(X, ys, rcond=None)
                    a, b = float(coef[0]), float(coef[1])
                except Exception:
                    XTX = X.T @ X + ridge * np.eye(2, dtype=X.dtype)
                    XTy = X.T @ ys
                    coef = np.linalg.solve(XTX, XTy)
                    a, b = float(coef[0]), float(coef[1])

                calib.params[f"k{k}_t{t}"] = {"a": a, "b": b}

    return calib


def apply_calibration_params(y_pred_disp, calib: CalibrationParams):
    yp = y_pred_disp.clone()
    B, K, T = yp.shape

    if calib.method == "per_k":
        for k in range(K):
            params = calib.params.get(f"k{k}", {"a": 1.0, "b": 0.0})
            yp[:, k, :] = params["a"] * yp[:, k, :] + params["b"]

    elif calib.method == "per_kt":
        for k in range(K):
            for t in range(T):
                params = calib.params.get(f"k{k}_t{t}", {"a": 1.0, "b": 0.0})
                yp[:, k, t] = params["a"] * yp[:, k, t] + params["b"]

    return yp


# ========================== ç‰©ç†æ¨¡å‹å‰å‘ä¼ æ’­ ==========================
def phys_forward_raw(static_8, tvals, phys_F, phys_I, ion_transform, allow_grad=False):
    phys_F.eval()
    phys_I.eval()
    with torch.set_grad_enabled(allow_grad):
        f = phys_F(static_8, tvals)
        i_z = phys_I(static_8, tvals)
        f_ch = f[:, 0:1, :]
        z_ch = i_z[:, 1:2, :] if i_z.size(1) >= 2 else i_z[:, 0:1, :]
        i = ion_transform(z_ch)
        phys = torch.cat([f_ch, i], dim=1)
        return torch.nan_to_num(phys, nan=0.0, posinf=1e6, neginf=-1e6)


def phys_interface_pipeline(phys_raw, variant, adapters):
    F = phys_raw[:, 0:1, :]
    I = phys_raw[:, 1:2, :]

    # Ionç­–ç•¥
    mode = variant.get("ion_gate", "use")
    if mode == "zero":
        I_eff = torch.zeros_like(I)
    elif mode == "const":
        I_eff = torch.nan_to_num(I, nan=0.0).mean(dim=0, keepdim=True).expand_as(I)
    elif mode == "smooth":
        k = 5
        pad = (k - 1) // 2
        w = torch.ones(1, 1, k, device=I.device) / k
        I_eff = nn.functional.conv1d(nn.functional.pad(I, (pad, pad), mode="replicate"), w)
    elif mode == "gate":
        I_eff = adapters["gate"](F, I)
    else:
        I_eff = I

    x = torch.cat([F, I_eff], dim=1)

    # æ´¾ç”Ÿç‰¹å¾
    if variant.get("use_derived", False):
        x = adapters["reducer"](F, I_eff)

    # PhysAdapter
    if variant.get("use_adapter", False):
        x = adapters["adapter"](x)

    return x


def train_single_variant_KFOLD(
    variant: Dict,
    data_dict: Dict,
    meta_old: Dict,
    device,
    seed: int,
    only_family: Optional[int] = None
):
    """
    5-FOLD äº¤å‰éªŒè¯è®­ç»ƒã€‚
    - è‹¥ only_family ä¸º Noneï¼šè”åˆ/Per-Head è®­ç»ƒï¼ˆä¸ Block A å¯¹åº”ï¼‰
    - è‹¥ only_family ä¸º intï¼šä»…å¯¹è¯¥ family çš„æœ‰æ•ˆæ ‡ç­¾è®­ç»ƒï¼ˆä¸ Block B å¯¹åº”ï¼‰
    è¿”å›å€¼ä¸ä¸»æµç¨‹æœŸæœ›ä¸€è‡´ï¼šåŒ…å« per_family / macro / micro / min_family ç­‰é”®ã€‚
    """
    from physio_util import set_seed, metrics, transform_for_display
    set_seed(seed)

    # -------- æ•°æ®é€‰æ‹©ï¼šæ•´åº“ or å• family æ©ç  --------
    fams_full: List[str] = data_dict["families"]
    if only_family is not None:
        fam_name = fams_full[only_family]
        dd = _masked_data_only_family(data_dict, only_family)
        v = _decorate_variant_name(variant, f"ONLY{fam_name}")
        print("\n" + "=" * 80)
        print(f"ğŸ”„ 5-FOLD CV Training (ONLY {fam_name}): {v['name']} | Seed: {seed}")
        print("=" * 80)
    else:
        dd = data_dict
        v = dict(variant)  # æµ…æ‹·è´
        print("\n" + "=" * 80)
        print(f"ğŸ”„ 5-FOLD CV Training: {v['name']} | Seed: {seed}")
        print("=" * 80)

    # -------- æå–æ•°æ® --------
    s8_full = dd["s8"]
    y_full = dd["y_sparse"]
    m_full = dd["m_sparse"]
    tvals_full = dd["tvals"]
    fams: List[str] = dd["families"]
    B = s8_full.shape[0]

    # åˆ†å±‚å˜é‡ï¼šæŒ‰æœ‰æ•ˆç‚¹å¤šå°‘è¿›è¡ŒäºŒåˆ†ç±»
    sample_counts = m_full.sum(dim=(1, 2)).numpy()
    strata = (sample_counts > np.median(sample_counts)).astype(int)

    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)
    indices = np.arange(B)

    fold_results = []  # ä¿å­˜æ¯æŠ˜çš„æŒ‡æ ‡ç­‰

    # -------- é€æŠ˜è®­ç»ƒ --------
    for fold, (train_idx, test_idx) in enumerate(kfold.split(indices, strata)):
        print(f"\n{'=' * 80}")
        print(f"ğŸ“Š Fold {fold + 1}/5: Train={len(train_idx)}, Test={len(test_idx)}")
        print(f"{'=' * 80}")
        s8_raw_all = dd["s8_raw"]  # (B, 7) åŸå§‹åŸŸï¼ˆæœªæ ‡å‡†åŒ–ï¼‰
        s8_train_raw = s8_raw_all[train_idx]
        train_mean = s8_train_raw.mean(dim=0)
        train_std = s8_train_raw.std(dim=0).clamp_min(1e-8)
        s8_renorm = (s8_raw_all - train_mean) / train_std

        # å½“å‰ fold çš„å¼ é‡
        s8_train = s8_renorm[train_idx].to(device)
        y_train = y_full[train_idx].to(device)
        m_train = m_full[train_idx].to(device)
        s8_test = s8_renorm[test_idx].to(device)
        y_test = y_full[test_idx].to(device)
        m_test = m_full[test_idx].to(device)

        tvals = (tvals_full if tvals_full.dim() == 1 else tvals_full[0]).to(device)
        T = len(tvals)
        K = len(fams)

        # ------ åˆå§‹åŒ–æ¨¡å‹ä¸æ¨¡å— ------
        phys_F, phys_I, ion_aff_init = build_phys_from_ckpt(
            Cfg.phys_ckpt_F, Cfg.phys_ckpt_I, device
        )
        freeze_physics_models(phys_F, phys_I, verbose=False)

        ion_tr = IonInverseTransform(
            ion_aff_init,
            learnable=v.get("learnable_ion", False)
        ).to(device)

        morph = TemporalRegressor(
            K=K, d_model=64, nhead=4, num_layers=2, dim_ff=128,
            dropout=Cfg.dropout_morph, T=T
        ).to(device)

        if os.path.exists(Cfg.morph_ckpt):
            ck = _safe_load(Cfg.morph_ckpt, map_location="cpu")
            sd = ck["model"] if isinstance(ck, dict) and "model" in ck else ck
            morph.load_state_dict(sd, strict=False)

        freeze_model_parts(
            morph,
            freeze_encoder=v.get("freeze_morph_encoder", True),
            freeze_heads=v.get("freeze_morph_heads", False),
            verbose=False
        )

        calib = build_calib_head(v["calib_head"], K=K).to(device)

        per_family_head = PerFamilyHead(K=K).to(device) \
            if v.get("per_family_head", False) else nn.Identity().to(device)

        hetero_head = HeteroHead(K=K, per_family=v.get("per_family_head", False)).to(device) \
            if v.get("hetero", False) else None

        task_logvars = nn.Parameter(torch.zeros(K, device=device)) \
            if v.get("task_uncertainty", False) else None

        adapters = {
            "adapter": PhysAdapter(2, k=3).to(device),
            "reducer": PhysFeaReducer().to(device),
            "gate": IonGate(k=5).to(device),
        }

        # ------ ä¼˜åŒ–å™¨ / è°ƒåº¦ / EMA ------
        param_groups = []
        morph_params = [p for p in morph.parameters() if p.requires_grad]
        if morph_params:
            param_groups.append({'params': morph_params, 'lr': Cfg.lr_morph, 'weight_decay': Cfg.wd_morph})
        calib_params = [p for p in calib.parameters() if p.requires_grad]
        if calib_params:
            param_groups.append({'params': calib_params, 'lr': Cfg.lr_calib, 'weight_decay': Cfg.wd_calib})
        if not isinstance(per_family_head, nn.Identity):
            param_groups.append({'params': per_family_head.parameters(), 'lr': Cfg.lr_morph, 'weight_decay': Cfg.wd_morph})
        if v.get("learnable_ion", False):
            param_groups.append({'params': ion_tr.parameters(), 'lr': Cfg.ion_learnable_lr, 'weight_decay': Cfg.ion_learnable_wd})
        for ad in adapters.values():
            ad_params = [p for p in ad.parameters() if p.requires_grad]
            if ad_params:
                param_groups.append({'params': ad_params, 'lr': Cfg.lr_morph, 'weight_decay': Cfg.wd_morph})

        optimizer = torch.optim.AdamW(param_groups)
        if Cfg.use_scheduler:
            def lr_lambda(epoch):
                if epoch < Cfg.warmup_epochs:
                    return epoch / Cfg.warmup_epochs
                progress = (epoch - Cfg.warmup_epochs) / (Cfg.max_epochs - Cfg.warmup_epochs)
                return Cfg.min_lr_ratio + 0.5 * (1 - Cfg.min_lr_ratio) * (1 + np.cos(np.pi * progress))
            scheduler = LambdaLR(optimizer, lr_lambda)
        else:
            scheduler = None

        ema = EMA(morph, decay=Cfg.ema_decay) if Cfg.use_ema else None

        # ------ è®­ç»ƒå¾ªç¯ ------
        best_train_loss = float('inf')
        for epoch in range(1, Cfg.max_epochs + 1):
            morph.train()
            optimizer.zero_grad()

            phys_raw = phys_forward_raw(s8_train, tvals, phys_F, phys_I, ion_tr, allow_grad=False)
            phys_enh = phys_interface_pipeline(phys_raw, v, adapters)
            y_pred = morph(s8_train, phys_enh, tvals)
            if v.get("per_family_head", False):
                y_pred = per_family_head(y_pred)
            y_pred = calib(y_pred)

            if v.get("hetero", False) and hetero_head is not None:
                y_mu, y_logvar = hetero_head(y_pred)
                loss = hetero_nll(y_mu, y_logvar, y_train, m_train, task_logvars)
            else:
                mono_penalty = None
                if "zmin" in fams:
                    mono_penalty = {"k_idx": fams.index("zmin"), "weight": Cfg.mono_zmin_weight, "direction": "decrease"}
                loss, _ = masked_huber_with_channel_norm(
                    y_pred, y_train, m_train,
                    delta=Cfg.loss_delta, smooth_weight=Cfg.loss_smooth_weight,
                    mono_penalty=mono_penalty
                )

            loss.backward()
            torch.nn.utils.clip_grad_norm_(morph.parameters(), Cfg.batch_clip)
            optimizer.step()
            if ema is not None: ema.update(morph)
            if scheduler is not None: scheduler.step()
            if loss.item() < best_train_loss:
                best_train_loss = loss.item()
            if epoch % 100 == 0 or epoch == 1:
                print(f"  Epoch {epoch}/{Cfg.max_epochs}: Loss={loss.item():.4f}")

        # ------ æµ‹è¯•è¯„ä¼°ï¼ˆä½¿ç”¨ EMA æƒé‡ï¼‰ ------
        morph.eval()
        with torch.no_grad():
            if ema is not None: ema.apply_to(morph)
            phys_test = phys_forward_raw(s8_test, tvals, phys_F, phys_I, ion_tr, allow_grad=False)
            phys_test_enh = phys_interface_pipeline(phys_test, v, adapters)
            y_test_pred = morph(s8_test, phys_test_enh, tvals)
            if v.get("per_family_head", False):
                y_test_pred = per_family_head(y_test_pred)
            y_test_pred = calib(y_test_pred)

            yhat_disp, ytrue_disp = transform_for_display(
                y_test_pred, y_test,
                family_sign=Cfg.family_sign, unit_scale=Cfg.unit_scale,
                flip_sign=Cfg.flip_sign, clip_nonneg=Cfg.clip_nonneg,
                min_display_value=Cfg.min_display_value
            )
            mts = metrics(yhat_disp, ytrue_disp, m_test)
            per_family, macro, micro, min_family = compute_per_family_metrics(mts, m_test, fams)
            if ema is not None: ema.restore(morph)

        fold_results.append({
            'fold': fold,
            'train_idx': train_idx.tolist(),
            'test_idx': test_idx.tolist(),
            'test_metrics': {
                'R2_macro': macro.get('R2', np.nan),
                'MAE_macro': macro.get('MAE', np.nan),
                'RMSE_macro': macro.get('RMSE', np.nan),
                # ä»…ä¿ç•™æ¯ family çš„ R2ï¼Œè¶³å¤Ÿç”¨äºæ±‡æ€»ä¸ min_family è®¡ç®—
                'R2_per_family': {fam: {'R2': per_family[fam].get('R2', np.nan)} for fam in fams},
            },
            'train_loss_final': best_train_loss,
        })

        # é‡Šæ”¾æ˜¾å­˜
        del morph, phys_F, phys_I, optimizer, scheduler, ema
        torch.cuda.empty_cache()

    # -------- èšåˆ 5 æŠ˜ç»“æœï¼Œè¿”å›ä¸ä¸»æµç¨‹ä¸€è‡´çš„ç»“æ„ --------
    r2_values = [f['test_metrics']['R2_macro'] for f in fold_results]
    mae_values = [f['test_metrics']['MAE_macro'] for f in fold_results]
    rmse_values = [f['test_metrics']['RMSE_macro'] for f in fold_results]
    r2_mean, r2_std = np.nanmean(r2_values), np.nanstd(r2_values)
    mae_mean, mae_std = np.nanmean(mae_values), np.nanstd(mae_values)
    rmse_mean, rmse_std = np.nanmean(rmse_values), np.nanstd(rmse_values)

    # æ¯ä¸ª family çš„ R2 å– 5 æŠ˜å‡å€¼
    per_family_agg = {}
    for fam in fams:
        r2_list = []
        for fr in fold_results:
            fam_r2 = fr['test_metrics']['R2_per_family'].get(fam, {}).get('R2', np.nan)
            if not np.isnan(fam_r2): r2_list.append(fam_r2)
        per_family_agg[fam] = {'R2': float(np.nanmean(r2_list)) if r2_list else np.nan}

    # macro / microï¼ˆmicro æ­¤å¤„ä¸å‚ä¸åç»­æ±‡æ€»ï¼Œå¯ç•™ç©ºå£³ï¼‰
    macro_agg = {'R2': float(r2_mean), 'MAE': float(mae_mean), 'RMSE': float(rmse_mean)}
    micro_agg = {}  # å ä½ï¼Œä¸å½±å“ summary_report çš„ä½¿ç”¨

    # æœ€å·® familyï¼ˆR2 æœ€ä½ï¼‰
    r2_map = {fam: per_family_agg[fam].get('R2', -np.inf) for fam in fams}
    min_family = {'R2': min(r2_map, key=r2_map.get)}

    # ä¿å­˜ä¸€ä¸ªèšåˆ JSONï¼ˆä¸åŸé€»è¾‘ä¸€è‡´ï¼‰
    save_root = os.path.join(Cfg.save_root, f"{v['name']}_seed{seed}")
    os.makedirs(save_root, exist_ok=True)
    final_results = {
        'variant_name': v['name'],
        'seed': seed,
        'n_folds': 5,
        'fold_results': fold_results,
        'aggregated_metrics': {
            'R2_macro_mean': r2_mean, 'R2_macro_std': r2_std,
            'MAE_macro_mean': mae_mean, 'MAE_macro_std': mae_std,
            'RMSE_macro_mean': rmse_mean, 'RMSE_macro_std': rmse_std,
        },
    }
    with open(os.path.join(save_root, 'kfold_results.json'), 'w') as f:
        json.dump(final_results, f, indent=2)
    print(f"[âœ“] Results saved to: {os.path.join(save_root, 'kfold_results.json')}\n")

    # â€”â€” å…³é”®ï¼šæŒ‰ä¸»æµç¨‹éœ€è¦è¿”å›å››å…ƒç»„ä¿¡æ¯ â€”â€” #
    return {
        'per_family': per_family_agg,
        'macro': macro_agg,
        'micro': micro_agg,
        'min_family': min_family,
        'variant_name': v['name'],
    }

def masked_huber_with_channel_norm(y_pred, y_true, mask, delta=1.0,
                                   smooth_weight=1e-3, mono_penalty=None):
    y_pred = torch.nan_to_num(y_pred, nan=0.0, posinf=1e6, neginf=-1e6)
    y_true = torch.nan_to_num(y_true, nan=0.0, posinf=1e6, neginf=-1e6)
    B, K, T = y_pred.shape
    device = y_pred.device

    # æœ‰æ•ˆæ©ç 
    finite_mask = torch.isfinite(y_true) & torch.isfinite(y_pred)
    eff_mask = (mask.bool() & finite_mask).float()

    # é€é€šé“è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    mean_k = torch.zeros(K, device=device)
    std_k = torch.ones(K, device=device)

    with torch.no_grad():
        for k in range(K):
            mk = eff_mask[:, k, :].bool()
            if mk.any():
                vt = y_true[:, k, :][mk]
                if vt.numel() > 1:
                    m = vt.mean()
                    s = vt.std()
                    if not torch.isfinite(m):
                        m = torch.tensor(0.0, device=device)
                    if not torch.isfinite(s):
                        s = torch.tensor(1.0, device=device)
                    mean_k[k] = m
                    std_k[k] = s.clamp_min(1.0)

    # æ ‡å‡†åŒ–
    y_true_n = torch.nan_to_num(
        (y_true - mean_k.view(1, K, 1)) / std_k.view(1, K, 1),
        nan=0.0, posinf=1e6, neginf=-1e6
    )
    y_pred_n = torch.nan_to_num(
        (y_pred - mean_k.view(1, K, 1)) / std_k.view(1, K, 1),
        nan=0.0, posinf=1e6, neginf=-1e6
    )

    # HuberæŸå¤±
    diff = (y_pred_n - y_true_n)
    absd = diff.abs()
    huber = torch.where(
        absd <= delta,
        0.5 * diff * diff,
        delta * (absd - 0.5 * delta)
    )

    # é€é€šé“å¹³å‡
    denom_k = eff_mask.sum(dim=(0, 2)).clamp_min(1.0)
    loss_main_per_k = (huber * eff_mask).sum(dim=(0, 2)) / denom_k

    # æŒ‰æ ‡å‡†å·®åŠ æƒï¼ˆæ ‡å‡†å·®å¤§çš„familyæƒé‡å°ï¼‰
    w_k = 1.0 / std_k.clamp_min(1.0)
    loss_main = (loss_main_per_k * w_k).mean()

    # å¹³æ»‘æ­£åˆ™åŒ–ï¼ˆäºŒé˜¶å·®åˆ†ï¼‰
    loss_smooth = torch.tensor(0.0, device=device)
    if T >= 3 and smooth_weight > 0:
        d1 = y_pred_n[:, :, 1:] - y_pred_n[:, :, :-1]  # ä¸€é˜¶å·®åˆ†
        d2 = d1[:, :, 1:] - d1[:, :, :-1]  # äºŒé˜¶å·®åˆ†
        loss_smooth = torch.nan_to_num((d2 ** 2).mean(), nan=0.0, posinf=0.0, neginf=0.0)

    # å•è°ƒæ€§çº¦æŸ
    loss_mono = torch.tensor(0.0, device=device)
    if mono_penalty is not None and T >= 2:
        k_idx = mono_penalty.get("k_idx", None)
        w_mono = mono_penalty.get("weight", 0.0)
        direction = mono_penalty.get("direction", "decrease")

        if (k_idx is not None) and (w_mono > 0):
            d = y_pred[:, k_idx, 1:] - y_pred[:, k_idx, :-1]
            if direction == "decrease":
                # è¦æ±‚å•è°ƒé€’å‡ï¼Œæƒ©ç½šæ­£çš„å·®åˆ†
                loss_mono = torch.nn.functional.relu(d).mean() * w_mono
            else:
                # è¦æ±‚å•è°ƒé€’å¢ï¼Œæƒ©ç½šè´Ÿçš„å·®åˆ†
                loss_mono = torch.nn.functional.relu(-d).mean() * w_mono

    # æ€»æŸå¤±
    loss = loss_main + smooth_weight * loss_smooth + loss_mono

    # ç¡®ä¿æŸå¤±æœ‰æ•ˆ
    if not torch.isfinite(loss):
        loss = torch.tensor(0.0, device=device)

    loss_dict = {
        "loss_main": loss_main.detach(),
        "loss_smooth": loss_smooth.detach(),
        "loss_mono": loss_mono.detach(),
        "loss_total": loss.detach()
    }

    return loss, loss_dict


def hetero_nll(y_mu, y_logvar, y_true, mask, task_logvars=None):

    y_true = torch.nan_to_num(y_true, nan=0.0, posinf=1e6, neginf=-1e6)
    m = (mask.bool() & torch.isfinite(y_true)).float()

    # æ·»åŠ ä»»åŠ¡ä¸ç¡®å®šæ€§
    if task_logvars is not None:
        y_logvar = y_logvar + task_logvars.view(1, -1, 1)

    # è´Ÿå¯¹æ•°ä¼¼ç„¶
    inv_var = torch.exp(-y_logvar).clamp_max(1e6)
    nll = 0.5 * ((y_mu - y_true) ** 2 * inv_var + y_logvar)

    # æŒ‰æœ‰æ•ˆç‚¹å¹³å‡
    nll = (nll * m).sum(dim=(0, 2)) / m.sum(dim=(0, 2)).clamp_min(1.0)

    return nll.mean()


# ========================== å­¦ä¹ ç‡è°ƒåº¦å™¨ ==========================
def create_lr_scheduler_with_warmup(optimizer, max_epochs, warmup_epochs=20,
                                    min_lr_ratio=0.01):
    def lr_lambda(epoch):
        if epoch < warmup_epochs:
            # Warmupé˜¶æ®µï¼šçº¿æ€§å¢é•¿
            return (epoch + 1) / warmup_epochs
        else:
            # ä½™å¼¦é€€ç«é˜¶æ®µ
            progress = (epoch - warmup_epochs) / (max_epochs - warmup_epochs)
            cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))
            return min_lr_ratio + (1 - min_lr_ratio) * cosine_decay

    return LambdaLR(optimizer, lr_lambda)


def get_current_lr(optimizer):
    """è·å–å½“å‰å­¦ä¹ ç‡"""
    return optimizer.param_groups[0]['lr']


# ========================== è®­ç»ƒè¾…åŠ©å‡½æ•° ==========================
def compute_gradient_norm(model):
    total_norm = 0.0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    total_norm = total_norm ** 0.5
    return total_norm


def clip_gradients(model, max_norm=1.0):
    return torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)


def count_parameters(model, trainable_only=True):
    if trainable_only:
        return sum(p.numel() for p in model.parameters() if p.requires_grad)
    else:
        return sum(p.numel() for p in model.parameters())


def get_model_device(model):
    """è·å–æ¨¡å‹æ‰€åœ¨è®¾å¤‡"""
    return next(model.parameters()).device


# ========================== æ‰¹æ¬¡å¤„ç† ==========================
def prepare_batch(batch, device):
    return tuple(x.to(device) if torch.is_tensor(x) else x for x in batch)


def collate_sparse_batch(samples):
    s8_list, y_list, m_list, t_list = zip(*samples)

    s8 = torch.stack(s8_list, dim=0)
    y_sparse = torch.stack(y_list, dim=0)
    m_sparse = torch.stack(m_list, dim=0)

    # æ—¶é—´å€¼å¯èƒ½æ˜¯å…±äº«çš„
    if len(set([tuple(t.tolist()) for t in t_list])) == 1:
        tvals = t_list[0]
    else:
        tvals = torch.stack(t_list, dim=0)

    return s8, y_sparse, m_sparse, tvals


# ========================== è®­ç»ƒæ—¥å¿— ==========================
class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—è®°å½•å™¨"""

    def __init__(self, log_interval=10):
        self.log_interval = log_interval
        self.epoch_losses = []
        self.epoch_metrics = []

    def log_step(self, epoch, step, total_steps, loss, lr):
        """è®°å½•è®­ç»ƒæ­¥éª¤"""
        if step % self.log_interval == 0 or step == total_steps - 1:
            print(f"[Epoch {epoch}] Step {step}/{total_steps} | "
                  f"Loss: {loss:.4f} | LR: {lr:.6f}")

    def log_epoch(self, epoch, train_loss, val_metrics, best_metric=None):
        """è®°å½•epochç»“æœ"""
        self.epoch_losses.append(train_loss)
        self.epoch_metrics.append(val_metrics)

        msg = f"[Epoch {epoch}] Train Loss: {train_loss:.4f}"

        if val_metrics:
            for k, v in val_metrics.items():
                msg += f" | Val {k}: {v:.4f}"

        if best_metric is not None:
            msg += f" | Best: {best_metric:.4f}"

        print(msg)

    def get_summary(self):
        """è·å–è®­ç»ƒæ‘˜è¦"""
        return {
            "epoch_losses": self.epoch_losses,
            "epoch_metrics": self.epoch_metrics,
            "best_loss": min(self.epoch_losses) if self.epoch_losses else None,
        }


# ========================== æƒé‡åˆå§‹åŒ– ==========================
def initialize_weights(model, method='xavier'):
    for name, param in model.named_parameters():
        if 'weight' in name and param.dim() >= 2:
            if method == 'xavier':
                nn.init.xavier_uniform_(param)
            elif method == 'kaiming':
                nn.init.kaiming_normal_(param, mode='fan_out', nonlinearity='relu')
            elif method == 'normal':
                nn.init.normal_(param, mean=0, std=0.02)
        elif 'bias' in name:
            nn.init.zeros_(param)


# ========================== æ£€æŸ¥ç‚¹ç®¡ç† ==========================
def save_checkpoint(save_path, model, optimizer, scheduler, epoch,
                    metrics, ema=None, variant=None):
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'metrics': metrics,
        'variant': variant,
    }

    if ema is not None:
        checkpoint['ema_shadow'] = ema.shadow

    torch.save(checkpoint, save_path)


def load_checkpoint(load_path, model, optimizer=None, scheduler=None,
                    ema=None, device='cpu'):
    checkpoint = torch.load(load_path, map_location=device)

    model.load_state_dict(checkpoint['model_state_dict'])

    if optimizer and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

    if ema and 'ema_shadow' in checkpoint:
        ema.shadow = checkpoint['ema_shadow']

    return checkpoint


# ========================== æ¨¡å‹æ¨ç† ==========================
@torch.no_grad()
def predict_batch(model, s8, phys, tvals, use_ema=False, ema=None):
    model.eval()

    # ä¸´æ—¶åº”ç”¨EMAæƒé‡
    if use_ema and ema is not None:
        # ä¿å­˜åŸå§‹æƒé‡
        original_state = {n: p.data.clone() for n, p in model.named_parameters() if p.requires_grad}
        ema.apply_to(model)

    # é¢„æµ‹
    pred = model(s8, phys, tvals)

    # æ¢å¤åŸå§‹æƒé‡
    if use_ema and ema is not None:
        for n, p in model.named_parameters():
            if n in original_state:
                p.data.copy_(original_state[n])

    return pred


def compute_per_family_metrics(mts: Dict, m_sparse: torch.Tensor, fams: List[str]):
    # ç›‘ç£ä¿¡å·ï¼šæ¯ä¸ª(k,t)ä½ç½®æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
    sup = m_sparse.sum(dim=0).detach().cpu().numpy() > 0  # (K, T)

    # é€Familyç»Ÿè®¡
    per_family = {}
    for k, fam in enumerate(fams):
        per_family[fam] = {}
        for metric_name in ["R2", "MAE", "RMSE", "SMAPE", "MAPE", "MSE"]:
            if metric_name not in mts:
                continue

            grid = mts[metric_name]
            if torch.is_tensor(grid):
                grid = grid.detach().cpu().numpy()

            # è¯¥Familyçš„å¹³å‡å€¼ï¼ˆå¿½ç•¥æ— ç›‘ç£çš„æ—¶é—´ç‚¹ï¼‰
            fam_vals = grid[k, :]
            fam_sup = sup[k, :]

            if fam_sup.sum() > 0:
                per_family[fam][metric_name] = float(np.nanmean(fam_vals[fam_sup]))
            else:
                per_family[fam][metric_name] = np.nan

    # Macroå¹³å‡ï¼ˆå…¬å¹³å¯¹å¾…æ¯ä¸ªFamilyï¼‰
    macro = {}
    for metric_name in ["R2", "MAE", "RMSE", "SMAPE", "MAPE", "MSE"]:
        vals = [per_family[fam].get(metric_name, np.nan) for fam in fams]
        macro[metric_name] = float(np.nanmean(vals))

    # Microå¹³å‡ï¼ˆæŒ‰æ ·æœ¬æ•°åŠ æƒï¼‰
    micro = {}
    for metric_name in ["R2", "MAE", "RMSE", "SMAPE", "MAPE", "MSE"]:
        if metric_name not in mts:
            continue

        grid = mts[metric_name]
        if torch.is_tensor(grid):
            grid = grid.detach().cpu().numpy()

        # å…¨å±€å¹³å‡ï¼ˆæ‰€æœ‰æœ‰ç›‘ç£çš„ç‚¹ï¼‰
        if sup.sum() > 0:
            micro[metric_name] = float(np.nanmean(grid[sup]))
        else:
            micro[metric_name] = np.nan

    # æœ€å·®Familyï¼ˆRÂ²æœ€ä½ / MAEæœ€é«˜ï¼‰
    min_family = {}
    r2_vals = {fam: per_family[fam].get("R2", -np.inf) for fam in fams}
    min_family["R2"] = min(r2_vals, key=r2_vals.get)

    mae_vals = {fam: per_family[fam].get("MAE", np.inf) for fam in fams}
    min_family["MAE"] = max(mae_vals, key=mae_vals.get)

    return per_family, macro, micro, min_family


def print_per_family_report(per_family, macro, micro, min_family, fams,
                            title="Evaluation"):
    """â˜… æ‰“å°é€FamilyæŠ¥å‘Š"""
    print(f"\n{'=' * 80}")
    print(f"{title:^80}")
    print(f"{'=' * 80}")

    # é€Family
    print(f"\n{'Family':<10} {'RÂ²':>8} {'MAE':>8} {'RMSE':>8} {'SMAPE/MAPE':>12}")
    print("-" * 80)
    for fam in fams:
        r2 = per_family[fam].get("R2", np.nan)
        mae = per_family[fam].get("MAE", np.nan)
        rmse = per_family[fam].get("RMSE", np.nan)
        smape = per_family[fam].get("SMAPE", per_family[fam].get("MAPE", np.nan))
        print(f"{fam:<10} {r2:>8.4f} {mae:>8.2f} {rmse:>8.2f} {smape:>12.2f}")

    # æ±‡æ€»
    print("-" * 80)
    print(f"{'Macro Avg':<10} {macro.get('R2', np.nan):>8.4f} "
          f"{macro.get('MAE', np.nan):>8.2f} "
          f"{macro.get('RMSE', np.nan):>8.2f} "
          f"{macro.get('SMAPE', macro.get('MAPE', np.nan)):>12.2f}")
    print(f"{'Micro Avg':<10} {micro.get('R2', np.nan):>8.4f} "
          f"{micro.get('MAE', np.nan):>8.2f} "
          f"{micro.get('RMSE', np.nan):>8.2f} "
          f"{micro.get('SMAPE', micro.get('MAPE', np.nan)):>12.2f}")
    print(f"{'Min Family':<10} {min_family['R2']:<8} "
          f"(RÂ²={per_family[min_family['R2']].get('R2', np.nan):.4f})")
    print(f"{'=' * 80}\n")


# ========================== å¯è§†åŒ–å¢å¼º ==========================
def plot_per_family_diagnostics(y_pred, y_true, mask, fams, T_values,
                                save_dir, title_prefix=""):
    ensure_dir(save_dir)

    yp = y_pred.detach().cpu().numpy()
    yt = y_true.detach().cpu().numpy()
    mk = mask.detach().cpu().numpy()

    K = len(fams)

    # åˆ›å»ºå­å›¾ï¼š2è¡Œ Ã— Kåˆ—
    fig, axes = plt.subplots(2, K, figsize=(4 * K, 8))
    if K == 1:
        axes = axes.reshape(2, 1)

    for k, fam in enumerate(fams):
        valid = mk[:, k, :].flatten()
        yp_k = yp[:, k, :].flatten()[valid]
        yt_k = yt[:, k, :].flatten()[valid]

        if len(yp_k) == 0:
            continue

        # =============== ä¸Šæ’ï¼šParity Plot ===============
        ax1 = axes[0, k]
        ax1.scatter(yt_k, yp_k, alpha=0.5, s=10, c='blue')

        # å›å½’çº¿
        if len(yp_k) > 1:
            try:
                from scipy.stats import linregress
                slope, intercept, r, *_ = linregress(yt_k, yp_k)
                x_line = np.linspace(yt_k.min(), yt_k.max(), 100)
                ax1.plot(x_line, slope * x_line + intercept, 'r-',
                         label=f'y={slope:.2f}x+{intercept:.1f}\nRÂ²={r ** 2:.3f}')
            except:
                pass

        # ç†æƒ³çº¿ (y=x)
        ax1.plot([yt_k.min(), yt_k.max()], [yt_k.min(), yt_k.max()],
                 'k--', label='Ideal')
        ax1.set_xlabel('True')
        ax1.set_ylabel('Pred')
        ax1.set_title(f'{fam}')
        ax1.legend(fontsize=8)
        ax1.grid(alpha=0.3)

        # =============== ä¸‹æ’ï¼šResidual Histogram ===============
        ax2 = axes[1, k]
        residuals = yp_k - yt_k
        ax2.hist(residuals, bins=30, edgecolor='black', alpha=0.7, color='skyblue')
        ax2.axvline(0, color='red', linestyle='--', linewidth=2)
        ax2.set_xlabel('Residual')
        ax2.set_ylabel('Frequency')
        ax2.set_title(f'Î¼={residuals.mean():.2f}, Ïƒ={residuals.std():.2f}')
        ax2.grid(alpha=0.3)

    plt.tight_layout()
    save_path = os.path.join(save_dir, f"{title_prefix}_family_diagnostics.png")
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"[Plot] Saved diagnostics to {save_path}")


def plot_temporal_error(y_pred, y_true, mask, fams, T_values,
                        save_dir, title_prefix=""):
    ensure_dir(save_dir)

    yp = y_pred.detach().cpu().numpy()
    yt = y_true.detach().cpu().numpy()
    mk = mask.detach().cpu().numpy().astype(bool)

    K = len(fams)
    T = len(T_values)

    # åˆ›å»ºå­å›¾ï¼šKè¡Œ Ã— 1åˆ—
    fig, axes = plt.subplots(K, 1, figsize=(10, 3 * K), squeeze=False)

    for k, fam in enumerate(fams):
        ax = axes[k, 0]
        mae_t = []

        for t in range(T):
            valid = mk[:, k, t]
            if valid.sum() == 0:
                mae_t.append(np.nan)
                continue

            err = np.abs(yp[:, k, t][valid] - yt[:, k, t][valid])
            mae_t.append(float(err.mean()))

        # ç»˜åˆ¶MAEæ›²çº¿
        ax.plot(T_values, mae_t, marker='o', linewidth=2,
                markersize=8, label=fam, color='steelblue')
        ax.fill_between(T_values, mae_t, alpha=0.3, color='steelblue')

        ax.set_xlabel('Time', fontsize=12)
        ax.set_ylabel('MAE', fontsize=12)
        ax.set_title(f'{fam} - Temporal MAE', fontsize=14, fontweight='bold')
        ax.grid(alpha=0.3)
        ax.legend()

    plt.tight_layout()
    save_path = os.path.join(save_dir, f"{title_prefix}_temporal_error.png")
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"[Plot] Saved temporal error to {save_path}")


def plot_training_curves(train_losses, val_metrics, save_dir, title_prefix=""):
    ensure_dir(save_dir)

    epochs = list(range(1, len(train_losses) + 1))

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # å·¦å›¾ï¼šè®­ç»ƒæŸå¤±
    ax1 = axes[0]
    ax1.plot(epochs, train_losses, marker='o', linewidth=2, label='Train Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.set_title('Training Loss')
    ax1.legend()
    ax1.grid(alpha=0.3)

    # å³å›¾ï¼šéªŒè¯RÂ²
    ax2 = axes[1]
    if val_metrics and len(val_metrics) > 0:
        val_r2 = [m.get('R2_macro', np.nan) for m in val_metrics]
        ax2.plot(epochs, val_r2, marker='s', linewidth=2,
                 color='green', label='Val RÂ² (Macro)')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('RÂ²')
        ax2.set_title('Validation RÂ²')
        ax2.legend()
        ax2.grid(alpha=0.3)

    plt.tight_layout()
    save_path = os.path.join(save_dir, f"{title_prefix}_training_curves.png")
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()

    print(f"[Plot] Saved training curves to {save_path}")


# ========================== ç»“æœä¿å­˜ ==========================
def save_evaluation_results(save_dir, per_family, macro, micro, min_family,
                            variant_name=""):
    ensure_dir(save_dir)

    results = {
        "variant": variant_name,
        "per_family": per_family,
        "macro": macro,
        "micro": micro,
        "min_family": min_family,
    }

    save_path = os.path.join(save_dir, f"{variant_name}_evaluation.json")
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"[Save] Evaluation results saved to {save_path}")

    return save_path


def save_predictions(save_dir, y_pred, y_true, mask, fams, T_values,
                     variant_name=""):
    ensure_dir(save_dir)

    save_dict = {
        'y_pred': y_pred.detach().cpu().numpy(),
        'y_true': y_true.detach().cpu().numpy(),
        'mask': mask.detach().cpu().numpy(),
        'families': fams,
        'time_values': T_values,
    }

    save_path = os.path.join(save_dir, f"{variant_name}_predictions.npz")
    np.savez_compressed(save_path, **save_dict)

    print(f"[Save] Predictions saved to {save_path}")

    return save_path


def generate_summary_report(save_dir, variant_results, fams):
    ensure_dir(save_dir)

    # åˆ›å»ºæ±‡æ€»è¡¨æ ¼
    report_lines = []
    report_lines.append("=" * 100)
    report_lines.append("VARIANT COMPARISON REPORT")
    report_lines.append("=" * 100)
    report_lines.append("")

    # è¡¨å¤´
    header = f"{'Variant':<30} {'RÂ²_Macro':>10} {'MAE_Macro':>10} {'RMSE_Macro':>10} {'Min_RÂ²_Family':>15}"
    report_lines.append(header)
    report_lines.append("-" * 100)

    # é€å˜ä½“
    best_r2 = -1e9
    best_variant = None

    for variant_name, (per_family, macro, micro, min_family) in variant_results.items():
        r2 = macro.get('R2', np.nan)
        mae = macro.get('MAE', np.nan)
        rmse = macro.get('RMSE', np.nan)
        min_fam = min_family.get('R2', 'N/A')

        line = f"{variant_name:<30} {r2:>10.4f} {mae:>10.2f} {rmse:>10.2f} {min_fam:>15}"
        report_lines.append(line)

        if not np.isnan(r2) and r2 > best_r2:
            best_r2 = r2
            best_variant = variant_name

    report_lines.append("-" * 100)
    report_lines.append(f"\nBest Variant: {best_variant} (RÂ²={best_r2:.4f})")
    report_lines.append("=" * 100)

    # ä¿å­˜æŠ¥å‘Š
    report_path = os.path.join(save_dir, "summary_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))

    # æ‰“å°æŠ¥å‘Š
    print('\n'.join(report_lines))
    print(f"\n[Save] Summary report saved to {report_path}")

    return report_path


def _safe_load(path, map_location="cpu"):
    """å®‰å…¨åŠ è½½checkpoint"""
    try:
        return torch.load(path, map_location=map_location)
    except Exception as e:
        print(f"[WARN] Failed to load {path}: {e}")
        return {}


def _infer_arch_from_sd(sd: dict) -> Dict[str, int]:
    # å…œåº•
    T_default, d_model_default = 10, 128

    # pos åµŒå…¥æ¨æ–­
    pos = sd.get("pos", None)
    if pos is not None and hasattr(pos, "shape") and pos.dim() == 3:
        _, T, d_model = pos.shape
    else:
        w = sd.get("input_proj.weight", None)
        if w is not None and hasattr(w, "shape"):
            d_model = w.shape[0]
        else:
            d_model = d_model_default
        T = T_default

    key_l1 = "encoder.layers.0.linear1.weight"
    if key_l1 in sd:
        dim_ff = sd[key_l1].shape[0]
    else:
        dim_ff = max(2 * d_model, 256)

    nhead = 8 if d_model % 8 == 0 else 4

    L = 0
    while any(k.startswith(f"encoder.layers.{L}.") for k in sd.keys()):
        L += 1
    num_layers = max(L, 1)

    return dict(T=T, d_model=d_model, nhead=nhead, dim_ff=dim_ff, num_layers=num_layers)

def build_phys_from_ckpt(ckpt_F_path, ckpt_I_path, device):
    ckf = _safe_load(ckpt_F_path, map_location="cpu")
    cki = _safe_load(ckpt_I_path, map_location="cpu")

    if not ckf or not cki:
        print(f"[WARN] Physics ckpt missing or empty. Using fallback tiny arch.")
        arch_F = arch_I = dict(T=10, d_model=128, nhead=4, dim_ff=256, num_layers=2)
        pf = PhysicsSeqPredictor(**arch_F).to(device)
        pi = PhysicsSeqPredictor(**arch_I).to(device)
        ion_aff = copy.deepcopy(Cfg.ion_affine_default)
        return pf, pi, ion_aff

    sd_F = ckf["model"] if isinstance(ckf, dict) and "model" in ckf else ckf
    sd_I = cki["model"] if isinstance(cki, dict) and "model" in cki else cki

    arch_F = _infer_arch_from_sd(sd_F) if sd_F else dict(T=10, d_model=128, nhead=4, dim_ff=256, num_layers=2)
    arch_I = _infer_arch_from_sd(sd_I) if sd_I else dict(T=10, d_model=128, nhead=4, dim_ff=256, num_layers=2)

    pf = PhysicsSeqPredictor(**arch_F).to(device)
    pi = PhysicsSeqPredictor(**arch_I).to(device)

    if sd_F: pf.load_state_dict(sd_F, strict=False)
    if sd_I: pi.load_state_dict(sd_I, strict=False)

    ion_aff = cki.get("ion_affine", copy.deepcopy(Cfg.ion_affine_default)) \
        if isinstance(cki, dict) else copy.deepcopy(Cfg.ion_affine_default)

    return pf, pi, ion_aff

# ========================== å•å˜ä½“è®­ç»ƒæµç¨‹ ==========================
def train_single_variant(variant: Dict, data_dict: Dict, meta_old: Dict,
                         device, seed: int):
    set_seed(seed)
    variant_name = variant['name']
    save_dir = os.path.join(Cfg.save_root, f"{variant_name}_seed{seed}")
    os.makedirs(save_dir, exist_ok=True)

    print("\n" + "=" * 80)
    print(f"Training Variant: {variant_name} | Seed: {seed}")
    print("=" * 80)
    print(f"Description: {variant['description']}")
    print(f"Expected RÂ²: {variant.get('expected_r2_range', 'N/A')}")
    print(f"Expected params: ~{variant['expected_trainable_params']:,}")

    # è§£åŒ…æ•°æ®
    s8_full = data_dict["s8"].to(device)
    y_full = data_dict["y_sparse"].to(device)
    m_full = data_dict["m_sparse"].to(device)
    tvals_full = data_dict["tvals"].to(device)
    fams = data_dict["families"]

    train_idx = data_dict["splits"]["train"]
    val_idx = data_dict["splits"]["val"]
    test_idx = data_dict["splits"]["test"]

    # æå–å­é›†
    s8_train = s8_full[train_idx]
    y_train = y_full[train_idx]
    m_train = m_full[train_idx]

    s8_val = s8_full[val_idx]
    y_val = y_full[val_idx]
    m_val = m_full[val_idx]

    s8_test = s8_full[test_idx]
    y_test = y_full[test_idx]
    m_test = m_full[test_idx]

    tvals = tvals_full if tvals_full.dim() == 1 else tvals_full[0]
    T = len(tvals)
    K = len(fams)

    print(f"\nData: Train={len(train_idx)}, Val={len(val_idx)}, Test={len(test_idx)}")

    # === æ¨¡å‹åˆå§‹åŒ– ===
    print("\n[1/6] Initializing models...")

    # ç‰©ç†æ¨¡å‹
    phys_F, phys_I, ion_aff_init = build_phys_from_ckpt(
        Cfg.phys_ckpt_F, Cfg.phys_ckpt_I, device
    )
    freeze_physics_models(phys_F, phys_I, verbose=True)

    # Ionå˜æ¢
    ion_tr = IonInverseTransform(
        ion_aff_init,
        learnable=variant.get("learnable_ion", False)
    ).to(device)

    # å½¢è²Œæ¨¡å‹
    morph = TemporalRegressor(
        K=K,
        d_model=64,  # ä¸stageBä¸€è‡´
        nhead=4,
        num_layers=2,
        dim_ff=128,
        dropout=Cfg.dropout_morph,
        T=T
    ).to(device)

    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    if os.path.exists(Cfg.morph_ckpt):
        ck = _safe_load(Cfg.morph_ckpt, map_location="cpu")
        sd = ck["model"] if isinstance(ck, dict) and "model" in ck else ck
        morph.load_state_dict(sd, strict=False)
        print(f"  [âœ“] Loaded morph pretrained weights")

    # å†»ç»“ç­–ç•¥
    freeze_model_parts(
        morph,
        freeze_encoder=variant.get("freeze_morph_encoder", True),
        freeze_heads=variant.get("freeze_morph_heads", False),
        verbose=True
    )

    # æ ¡å‡†å¤´
    calib = build_calib_head(variant["calib_head"], K=K).to(device)

    # è¾“å‡ºå¤´ï¼ˆå¯é€‰ï¼‰
    per_family_head = PerFamilyHead(K=K).to(device) \
        if variant.get("per_family_head", False) \
        else nn.Identity().to(device)

    hetero_head = HeteroHead(K=K, per_family=variant.get("per_family_head", False)).to(device) \
        if variant.get("hetero", False) \
        else None

    task_logvars = nn.Parameter(torch.zeros(K, device=device)) \
        if variant.get("task_uncertainty", False) \
        else None

    # é€‚é…å™¨
    adapters = {
        "adapter": PhysAdapter(2, k=3).to(device),
        "reducer": PhysFeaReducer().to(device),
        "gate": IonGate(k=5).to(device),
    }

    # æ‰“å°å‚æ•°ç»Ÿè®¡
    model_dict = {
        "phys_F": phys_F,
        "phys_I": phys_I,
        "ion_tr": ion_tr,
        "morph": morph,
        "calib": calib,
    }
    print_trainable_params(model_dict)

    # === ä¼˜åŒ–å™¨ ===
    print("\n[2/6] Setting up optimizer and scheduler...")

    params = [
        {"params": [p for p in morph.parameters() if p.requires_grad],
         "lr": Cfg.lr_morph, "weight_decay": Cfg.wd_morph},
        {"params": [p for p in calib.parameters() if p.requires_grad],
         "lr": Cfg.lr_calib, "weight_decay": Cfg.wd_calib},
    ]

    if ion_tr.learnable:
        params.append({
            "params": [p for p in ion_tr.parameters() if p.requires_grad],
            "lr": Cfg.ion_learnable_lr,
            "weight_decay": Cfg.ion_learnable_wd
        })

    if variant.get("per_family_head", False):
        params.append({
            "params": [p for p in per_family_head.parameters() if p.requires_grad],
            "lr": Cfg.lr_calib,
            "weight_decay": Cfg.wd_calib
        })

    if task_logvars is not None:
        params.append({
            "params": [task_logvars],
            "lr": Cfg.lr_calib,
            "weight_decay": 0.0
        })

    optimizer = torch.optim.AdamW(params)
    scheduler = create_lr_scheduler_with_warmup(
        optimizer,
        max_epochs=Cfg.max_epochs,
        warmup_epochs=Cfg.warmup_epochs,
        min_lr_ratio=Cfg.min_lr_ratio
    )

    scaler = torch.cuda.amp.GradScaler(enabled=False)  # 30æ ·æœ¬ä¸éœ€è¦AMP

    # EMAå’Œæ—©åœ
    ema = EMA(morph, decay=Cfg.ema_decay) if Cfg.use_ema else None
    early_stopper = EarlyStopper(patience=Cfg.early_stop_patience) \
        if Cfg.early_stop else None

    logger = TrainingLogger(log_interval=Cfg.log_interval)

    # === è®­ç»ƒå¾ªç¯ ===
    print(f"\n[3/6] Training for {Cfg.max_epochs} epochs...")
    print(f"Full batch training: {Cfg.use_full_batch}")

    best_val_r2 = -1e9
    train_losses = []
    val_metrics_list = []

    for epoch in range(1, Cfg.max_epochs + 1):
        # ========== è®­ç»ƒ ==========
        morph.train()

        # æ•°æ®å¢å¼º
        s8_aug = s8_train

        optimizer.zero_grad()

        # ç‰©ç†å‰å‘
        phys_raw = phys_forward_raw(s8_aug, tvals, phys_F, phys_I, ion_tr,
                                    allow_grad=False)

        # ç‰©ç†ç‰¹å¾å¢å¼º
        phys_aug = phys_raw

        # æ¥å£å¢å¼º
        phys_enh = phys_interface_pipeline(phys_aug, variant, adapters)

        # å½¢è²Œé¢„æµ‹
        y_pred = morph(s8_aug, phys_enh, tvals)

        # è¾“å‡ºå¤´
        if variant.get("per_family_head", False):
            y_pred = per_family_head(y_pred)

        # æ ¡å‡†
        y_pred = calib(y_pred)

        # æŸå¤±è®¡ç®—
        if variant.get("hetero", False) and hetero_head is not None:
            y_mu, y_logvar = hetero_head(y_pred)
            loss = hetero_nll(y_mu, y_logvar, y_train, m_train, task_logvars)
            loss_dict = {"loss_total": loss.detach()}
        else:
            # å•è°ƒæ€§çº¦æŸï¼ˆå¦‚æœéœ€è¦ï¼‰
            mono_penalty = None
            if "zmin" in fams:
                k_zmin = fams.index("zmin")
                mono_penalty = {
                    "k_idx": k_zmin,
                    "weight": Cfg.mono_zmin_weight,
                    "direction": "decrease"
                }

            loss, loss_dict = masked_huber_with_channel_norm(
                y_pred, y_train, m_train,
                delta=Cfg.loss_delta,
                smooth_weight=Cfg.loss_smooth_weight,
                mono_penalty=mono_penalty
            )

        # åå‘ä¼ æ’­
        loss.backward()
        torch.nn.utils.clip_grad_norm_(morph.parameters(), Cfg.batch_clip)
        optimizer.step()

        # EMAæ›´æ–°
        if ema is not None:
            ema.update(morph)

        scheduler.step()

        train_losses.append(loss.item())

        # ========== éªŒè¯ ==========
        if epoch % 10 == 0 or epoch == Cfg.max_epochs or epoch == 1:
            morph.eval()
            with torch.no_grad():
                # ä½¿ç”¨EMAæƒé‡
                if ema is not None:
                    ema.apply_to(morph)

                # éªŒè¯é›†å‰å‘
                phys_val = phys_forward_raw(s8_val, tvals, phys_F, phys_I, ion_tr,
                                            allow_grad=False)
                phys_val_enh = phys_interface_pipeline(phys_val, variant, adapters)
                y_val_pred = morph(s8_val, phys_val_enh, tvals)

                if variant.get("per_family_head", False):
                    y_val_pred = per_family_head(y_val_pred)

                y_val_pred = calib(y_val_pred)

                # è½¬æ¢åˆ°å±•ç¤ºç©ºé—´
                yhat_disp, ytrue_disp = transform_for_display(
                    y_val_pred, y_val,
                    family_sign=Cfg.family_sign,
                    unit_scale=Cfg.unit_scale,
                    flip_sign=Cfg.flip_sign,
                    clip_nonneg=Cfg.clip_nonneg,
                    min_display_value=Cfg.min_display_value
                )

                # è®¡ç®—æŒ‡æ ‡
                mts = metrics(yhat_disp, ytrue_disp, m_val)
                per_family, macro, micro, min_family = compute_per_family_metrics(
                    mts, m_val, fams
                )

                val_r2 = macro.get('R2', 0.0)
                val_metrics_list.append({"R2_macro": val_r2, "MAE_macro": macro.get('MAE', 0.0)})

                # æ—¥å¿—
                logger.log_epoch(
                    epoch, loss.item(),
                    {"R2": val_r2, "MAE": macro.get('MAE', 0.0)},
                    best_val_r2
                )

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_r2 > best_val_r2:
                    best_val_r2 = val_r2
                    save_checkpoint(
                        os.path.join(save_dir, "best_model.pth"),
                        morph, optimizer, scheduler, epoch,
                        {"val_r2": val_r2, "macro": macro},
                        ema, variant
                    )
                    print(f"  [âœ“] Saved best model (RÂ²={val_r2:.4f})")

                # æ—©åœæ£€æŸ¥
                if early_stopper is not None:
                    if not early_stopper.step(-val_r2):
                        print(f"[Early Stop] at epoch {epoch}")
                        break

    # === åæ ¡å‡† ===
    print("\n[4/6] Post-calibration on training set...")

    morph.eval()
    with torch.no_grad():
        # è®­ç»ƒé›†é¢„æµ‹
        phys_train = phys_forward_raw(s8_train, tvals, phys_F, phys_I, ion_tr,
                                      allow_grad=False)
        phys_train_enh = phys_interface_pipeline(phys_train, variant, adapters)
        y_train_pred = morph(s8_train, phys_train_enh, tvals)

        if variant.get("per_family_head", False):
            y_train_pred = per_family_head(y_train_pred)

        y_train_pred = calib(y_train_pred)

        # è½¬æ¢åˆ°å±•ç¤ºç©ºé—´
        yhat_train_disp, ytrue_train_disp = transform_for_display(
            y_train_pred, y_train,
            family_sign=Cfg.family_sign,
            unit_scale=Cfg.unit_scale
        )

        # æ‹Ÿåˆæ ¡å‡†å‚æ•°
        calib_params = fit_calibration_params(
            yhat_train_disp, ytrue_train_disp, m_train,
            method=variant.get("post_calib", "per_k"),
            min_points=Cfg.calib_min_points,
            ridge=Cfg.calib_ridge
        )

    # === æµ‹è¯•é›†è¯„ä¼° ===
    print("\n[5/6] Testing on test set...")

    morph.eval()
    with torch.no_grad():
        # æµ‹è¯•é›†é¢„æµ‹
        phys_test = phys_forward_raw(s8_test, tvals, phys_F, phys_I, ion_tr,
                                     allow_grad=False)
        phys_test_enh = phys_interface_pipeline(phys_test, variant, adapters)
        y_test_pred = morph(s8_test, phys_test_enh, tvals)

        if variant.get("per_family_head", False):
            y_test_pred = per_family_head(y_test_pred)

        y_test_pred = calib(y_test_pred)

        # è½¬æ¢åˆ°å±•ç¤ºç©ºé—´
        yhat_test_disp, ytrue_test_disp = transform_for_display(
            y_test_pred, y_test,
            family_sign=Cfg.family_sign,
            unit_scale=Cfg.unit_scale
        )

        # åº”ç”¨åæ ¡å‡†
        yhat_test_cal = apply_calibration_params(yhat_test_disp, calib_params)

        # è®¡ç®—æŒ‡æ ‡ï¼ˆæ ¡å‡†å‰ï¼‰
        mts_before = metrics(yhat_test_disp, ytrue_test_disp, m_test)
        pf_before, macro_before, micro_before, min_fam_before = \
            compute_per_family_metrics(mts_before, m_test, fams)

        # è®¡ç®—æŒ‡æ ‡ï¼ˆæ ¡å‡†åï¼‰
        mts_after = metrics(yhat_test_cal, ytrue_test_disp, m_test)
        pf_after, macro_after, micro_after, min_fam_after = \
            compute_per_family_metrics(mts_after, m_test, fams)

        # æ‰“å°æŠ¥å‘Š
        print_per_family_report(pf_before, macro_before, micro_before, min_fam_before,
                                fams, f"{variant_name} - Before Calibration")
        print_per_family_report(pf_after, macro_after, micro_after, min_fam_after,
                                fams, f"{variant_name} - After Calibration")

    # === ä¿å­˜ç»“æœ ===
    print("\n[6/6] Saving results and plots...")

    save_evaluation_results(save_dir, pf_after, macro_after, micro_after,
                            min_fam_after, variant_name)
    save_predictions(save_dir, yhat_test_cal, ytrue_test_disp, m_test,
                     fams, tvals.cpu().numpy(), variant_name)

    plot_per_family_diagnostics(yhat_test_cal, ytrue_test_disp, m_test,
                                fams, tvals.cpu().numpy(), save_dir, variant_name)
    plot_temporal_error(yhat_test_cal, ytrue_test_disp, m_test,
                        fams, tvals.cpu().numpy(), save_dir, variant_name)

    print(f"\n[âœ“] Variant {variant_name} completed!")
    print(f"Final Test RÂ² (Macro): {macro_after.get('R2', 0.0):.4f}")

    return {
        "variant_name": variant_name,
        "per_family": pf_after,
        "macro": macro_after,
        "micro": micro_after,
        "min_family": min_fam_after,
        "best_val_r2": best_val_r2,
        "save_dir": save_dir
    }


# ========================== ä¸»å‡½æ•° ==========================
# ========================== ä¸»å‡½æ•°ï¼ˆæ›¿æ¢ä¸ºè‡ªåŠ¨æ‰¹è·‘ç‰ˆï¼‰ ==========================
def main():
    """è‡ªåŠ¨è·‘ï¼š5æŠ˜(KFold) + ä¸‰åˆ†(TriSplit) Ã— {è”åˆ/Per-Head/é€Familyç‹¬ç«‹}ï¼Œè¦†ç›– Cfg.variants Ã— Cfg.seeds"""
    print_config_summary()

    # è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"\n[Device] Using: {device}")

    # æ—§è¡¨ metaï¼ˆç”¨äº StageC å¯¹é½ï¼‰
    print("\n[Loading] Old table meta...")
    from physio_util import excel_to_physics_dataset
    try:
        _, meta_old = excel_to_physics_dataset(Cfg.old_excel, sheet_name=Cfg.sheet_name)
        print(f"  [âœ“] Loaded meta from {Cfg.old_excel}")
    except Exception as e:
        print(f"  [âœ—] Failed to load meta: {e}")
        print("  [INFO] Using default meta")
        meta_old = {
            "norm_mean": torch.zeros(7),
            "norm_std": torch.ones(7),
            "time_values": [1, 3, 5, 9, 10],
            "families": ["zmin", "h1", "d1", "w", "h2", "d2"]
        }

    # æ–°è¡¨æ•°æ®ï¼ˆå«å›ºå®šä¸‰åˆ†åˆ’åˆ†ï¼‰
    print("\n[Loading] New table data...")
    data_dict = load_all_data(meta_old)
    fams: List[str] = data_dict["families"]

    # èšåˆæ‰€æœ‰ç»“æœï¼Œç”Ÿæˆå¤§æ±‡æ€»
    all_results = {}

    # === Sweep over variants Ã— seeds ===
    for variant in Cfg.variants:
        for seed in Cfg.seeds:
            # ----------------------------
            # A) 5-FOLD CVï¼ˆè”åˆ/Per-Headï¼‰
            # ----------------------------
            try:
                print("\n" + "="*80)
                print(f"[AUTO] KFold | JOINT/PER-HEAD | {variant['name']} | seed={seed}")
                kf_res = train_single_variant_KFOLD(variant, data_dict, meta_old, device, seed)
                key = f"{variant['name']}_seed{seed}_KFOLD"
                # ç»Ÿä¸€æ”¶é›†ä¸º (per_family, macro, micro, min_family)
                all_results[key] = (
                    kf_res['per_family'],
                    kf_res['macro'],
                    kf_res['micro'],
                    kf_res['min_family']
                )
            except Exception as e:
                print(f"\n[ERROR] KFOLD joint/per-head failed: {variant['name']} s{seed}: {e}")
                import traceback; traceback.print_exc()

            # ----------------------------
            # B) 5-FOLD CVï¼ˆé€ family ç‹¬ç«‹ï¼‰
            # ----------------------------
            for k, fam in enumerate(fams):
                try:
                    print("\n" + "-"*80)
                    print(f"[AUTO] KFold | SEPARATE-FAMILY={fam} | {variant['name']} | seed={seed}")
                    dd_k = _masked_data_only_family(data_dict, k)
                    v_k = _decorate_variant_name(variant, f"ONLY{fam}")
                    kf_sep_res = train_single_variant_KFOLD(v_k, dd_k, meta_old, device, seed)
                    key = f"{v_k['name']}_seed{seed}_KFOLD"
                    all_results[key] = (
                        kf_sep_res['per_family'],
                        kf_sep_res['macro'],
                        kf_sep_res['micro'],
                        kf_sep_res['min_family']
                    )
                except Exception as e:
                    print(f"\n[ERROR] KFOLD separate family failed: {variant['name']} [{fam}] s{seed}: {e}")
                    import traceback; traceback.print_exc()

            # ----------------------------
            # C) TriSplitï¼ˆè”åˆ/Per-Headï¼‰
            # æ³¨ï¼šTriSplit ç‰ˆæœ¬å·²ç”± train_single_variant å®ç°ï¼ˆä½¿ç”¨ data_dict['splits']ï¼‰
            # ä¸ºé¿å…ç›®å½•å†²çªï¼Œç»™ variant ä¸´æ—¶åŠ åç¼€ _TRI
            # ----------------------------
            try:
                print("\n" + "="*80)
                print(f"[AUTO] TRISPLIT | JOINT/PER-HEAD | {variant['name']} | seed={seed}")
                v_tri = _decorate_variant_name(variant, "TRI")
                tri_res = train_single_variant(v_tri, data_dict, meta_old, device, seed)
                key = f"{v_tri['name']}_seed{seed}"
                all_results[key] = (
                    tri_res['per_family'],
                    tri_res['macro'],
                    tri_res['micro'],
                    tri_res['min_family']
                )
            except Exception as e:
                print(f"\n[ERROR] TRISPLIT joint/per-head failed: {variant['name']} s{seed}: {e}")
                import traceback; traceback.print_exc()

            # ----------------------------
            # D) TriSplitï¼ˆé€ family ç‹¬ç«‹ï¼‰
            # ----------------------------
            for k, fam in enumerate(fams):
                try:
                    print("\n" + "-"*80)
                    print(f"[AUTO] TRISPLIT | SEPARATE-FAMILY={fam} | {variant['name']} | seed={seed}")
                    dd_k = _masked_data_only_family(data_dict, k)
                    v_tri_k = _decorate_variant_name(variant, f"TRI_ONLY{fam}")
                    tri_sep_res = train_single_variant(v_tri_k, dd_k, meta_old, device, seed)
                    key = f"{v_tri_k['name']}_seed{seed}"
                    all_results[key] = (
                        tri_sep_res['per_family'],
                        tri_sep_res['macro'],
                        tri_sep_res['micro'],
                        tri_sep_res['min_family']
                    )
                except Exception as e:
                    print(f"\n[ERROR] TRISPLIT separate family failed: {variant['name']} [{fam}] s{seed}: {e}")
                    import traceback; traceback.print_exc()

    # === æ±‡æ€»æŠ¥å‘Š ===
    if all_results:
        print("\n" + "=" * 80)
        print("Generating Summary Report")
        print("=" * 80)
        generate_summary_report(Cfg.save_root, all_results, fams)

    print("\n[âœ“] All auto-sweeps done!")
    print(f"Results saved under: {Cfg.save_root}")



# ========================== ç”¨äºæµ‹è¯•çš„ä¸»å‡½æ•° ==========================
if __name__ == "__main__":
    main()
