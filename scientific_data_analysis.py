# -*- coding: utf-8 -*-
"""
ç§‘å­¦æ•°æ®åˆ†æå·¥å…·ï¼šæ–°æ—§æ•°æ®å¯¹æ¯” + æ ·æœ¬å¢é‡è®¾è®¡
================================================

åŠŸèƒ½ï¼š
1. æ·±åº¦ç»Ÿè®¡åˆ†æï¼ˆå‡å€¼ã€æ–¹å·®ã€åˆ†å¸ƒæ£€éªŒï¼‰
2. å‚æ•°ç©ºé—´è¦†ç›–åº¦åˆ†æ
3. ç›¸å…³æ€§åˆ†æ
4. ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰
5. èšç±»åˆ†æ
6. æ ·æœ¬å……è¶³æ€§è¯„ä¼°
7. DOEï¼ˆå®éªŒè®¾è®¡ï¼‰å»ºè®®

è¾“å‡ºï¼š
- å®Œæ•´çš„PDFåˆ†ææŠ¥å‘Š
- è¯¦ç»†çš„ç»Ÿè®¡è¡¨æ ¼
- å¯è§†åŒ–å›¾è¡¨é›†
- DOEè®¾è®¡æ–¹æ¡ˆ
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.spatial.distance import cdist
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


# ============ é…ç½® ============
class Config:
    old_excel = r"D:\data\pycharm\bosch\case.xlsx"
    new_excel = r"D:\data\pycharm\bosch\Bosch.xlsx"
    old_sheet = "case"
    new_sheet = "Sheet1"
    output_dir = "./scientific_data_analysis"
    
    # åˆ—ååŒ¹é…è§„åˆ™
    key_alias = {
        "temp": ["temp", "temperature"],
        "apc": ["apc", "apc(e2", "apcï¼ˆe2"],
        "source_rf": ["source_rf", "sourcerf", "rfsource", "e2", "source_rf(e2", "source_rfï¼ˆe2"],
        "lf_rf": ["lf_rf", "lfrf", "bias", "lf_rf(e2", "lf_rfï¼ˆe2"],
        "sf6": ["sf6", "sf6(e2", "sf6ï¼ˆe2"],
        "c4f8": ["c4f8", "c4f8(dep", "c4f8ï¼ˆdep"],
        "dep_time": ["deptime", "dep_time", "dep time", "depositiontime"],
        "etch_time": ["etchtime", "etch_time", "etch time"],
    }
    
    param_names = ["APC", "SOURCE_RF", "LF_RF", "SF6", "C4F8", "DEP_TIME", "ETCH_TIME"]


# ============ è¾…åŠ©å‡½æ•° ============

def _canon(s: str) -> str:
    """æ ‡å‡†åŒ–åˆ—å"""
    import re
    s = str(s).strip().lower()
    s = re.sub(r"\s+", "", s)
    s = s.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
    return s


def _pick_one(df_cols, candidates):
    """ä»å€™é€‰åˆ—åä¸­åŒ¹é…"""
    cols_c = {c: _canon(c) for c in df_cols}
    for c in df_cols:
        v = cols_c[c]
        for pat in candidates:
            if pat in v:
                return c
    return None


def get_static_columns(df, key_alias):
    """è·å–recipeå‚æ•°åˆ—"""
    cols = list(df.columns)
    static_keys = []
    actual_names = []
    
    for key, aliases in key_alias.items():
        matched = _pick_one(cols, aliases)
        if matched:
            static_keys.append(matched)
            actual_names.append(key.upper())
        else:
            static_keys.append(None)
            actual_names.append(key.upper())
    
    return static_keys, actual_names


def load_data(excel_path, sheet_name, key_alias):
    """åŠ è½½å¹¶æ¸…æ´—æ•°æ®"""
    df = pd.read_excel(excel_path, sheet_name=sheet_name)
    static_keys, param_names = get_static_columns(df, key_alias)
    
    # æå–æ•°æ®
    data = []
    valid_names = []
    for col, name in zip(static_keys, param_names):
        if col is not None and col in df.columns:
            vals = df[col].values
            if not np.isnan(vals).all():
                data.append(vals)
                valid_names.append(name)
    
    if not data:
        return None, []
    
    data = np.array(data).T  # (N, K)
    
    # ç§»é™¤NaN
    valid_mask = ~np.isnan(data).any(axis=1)
    data = data[valid_mask]
    
    return data, valid_names


# ============ ç»Ÿè®¡åˆ†æ ============

def descriptive_statistics(old_data, new_data, param_names):
    """æè¿°æ€§ç»Ÿè®¡"""
    stats_list = []
    
    for i, param in enumerate(param_names):
        old_vals = old_data[:, i]
        new_vals = new_data[:, i]
        
        # åŸºç¡€ç»Ÿè®¡
        stats_dict = {
            'Parameter': param,
            'Old_N': len(old_vals),
            'Old_Mean': np.mean(old_vals),
            'Old_Std': np.std(old_vals),
            'Old_Min': np.min(old_vals),
            'Old_Q1': np.percentile(old_vals, 25),
            'Old_Median': np.median(old_vals),
            'Old_Q3': np.percentile(old_vals, 75),
            'Old_Max': np.max(old_vals),
            'Old_CV': np.std(old_vals) / (np.mean(old_vals) + 1e-8),  # å˜å¼‚ç³»æ•°
            'New_N': len(new_vals),
            'New_Mean': np.mean(new_vals),
            'New_Std': np.std(new_vals),
            'New_Min': np.min(new_vals),
            'New_Q1': np.percentile(new_vals, 25),
            'New_Median': np.median(new_vals),
            'New_Q3': np.percentile(new_vals, 75),
            'New_Max': np.max(new_vals),
            'New_CV': np.std(new_vals) / (np.mean(new_vals) + 1e-8),
        }
        
        # åˆ†å¸ƒå·®å¼‚
        stats_dict['Mean_Diff'] = stats_dict['New_Mean'] - stats_dict['Old_Mean']
        stats_dict['Std_Ratio'] = stats_dict['New_Std'] / (stats_dict['Old_Std'] + 1e-8)
        stats_dict['Range_Old'] = stats_dict['Old_Max'] - stats_dict['Old_Min']
        stats_dict['Range_New'] = stats_dict['New_Max'] - stats_dict['New_Min']
        stats_dict['Range_Ratio'] = stats_dict['Range_New'] / (stats_dict['Range_Old'] + 1e-8)
        
        # æ ‡å‡†åŒ–å·®å¼‚ï¼ˆCohen's dï¼‰
        pooled_std = np.sqrt((stats_dict['Old_Std']**2 + stats_dict['New_Std']**2) / 2)
        stats_dict['Cohens_d'] = stats_dict['Mean_Diff'] / (pooled_std + 1e-8)
        
        # ç»Ÿè®¡æ£€éªŒ
        # tæ£€éªŒï¼ˆå‡å€¼å·®å¼‚ï¼‰
        t_stat, p_value_t = stats.ttest_ind(old_vals, new_vals)
        stats_dict['t_statistic'] = t_stat
        stats_dict['p_value_ttest'] = p_value_t
        
        # Leveneæ£€éªŒï¼ˆæ–¹å·®é½æ€§ï¼‰
        levene_stat, p_value_levene = stats.levene(old_vals, new_vals)
        stats_dict['levene_statistic'] = levene_stat
        stats_dict['p_value_levene'] = p_value_levene
        
        # Kolmogorov-Smirnovæ£€éªŒï¼ˆåˆ†å¸ƒå·®å¼‚ï¼‰
        ks_stat, p_value_ks = stats.ks_2samp(old_vals, new_vals)
        stats_dict['ks_statistic'] = ks_stat
        stats_dict['p_value_ks'] = p_value_ks
        
        # åˆ†å¸ƒå½¢çŠ¶
        stats_dict['Old_Skewness'] = stats.skew(old_vals)
        stats_dict['Old_Kurtosis'] = stats.kurtosis(old_vals)
        stats_dict['New_Skewness'] = stats.skew(new_vals)
        stats_dict['New_Kurtosis'] = stats.kurtosis(new_vals)
        
        stats_list.append(stats_dict)
    
    return pd.DataFrame(stats_list)


def parameter_space_coverage(old_data, new_data, param_names):
    """å‚æ•°ç©ºé—´è¦†ç›–åº¦åˆ†æ"""
    coverage_list = []
    
    for i, param in enumerate(param_names):
        old_vals = old_data[:, i]
        new_vals = new_data[:, i]
        
        old_min, old_max = np.min(old_vals), np.max(old_vals)
        new_min, new_max = np.min(new_vals), np.max(new_vals)
        
        # è®¡ç®—æ–°æ•°æ®åœ¨æ—§æ•°æ®èŒƒå›´å†…çš„æ¯”ä¾‹
        in_range = np.sum((new_vals >= old_min) & (new_vals <= old_max))
        coverage_pct = 100 * in_range / len(new_vals)
        
        # è®¡ç®—æ–°æ•°æ®è¦†ç›–æ—§æ•°æ®èŒƒå›´çš„æ¯”ä¾‹
        old_range = old_max - old_min
        new_range = new_max - new_min
        range_coverage = 100 * new_range / (old_range + 1e-8)
        
        # ç©ºé—´é‡‡æ ·å¯†åº¦ï¼ˆæ ·æœ¬æ•°/èŒƒå›´ï¼‰
        old_density = len(old_vals) / (old_range + 1e-8)
        new_density = len(new_vals) / (new_range + 1e-8) if new_range > 1e-6 else 0
        
        coverage_list.append({
            'Parameter': param,
            'Old_Range': old_range,
            'New_Range': new_range,
            'Range_Coverage_%': range_coverage,
            'New_in_Old_Range_%': coverage_pct,
            'Old_Density': old_density,
            'New_Density': new_density,
            'Density_Ratio': new_density / (old_density + 1e-8),
            'Old_Min': old_min,
            'Old_Max': old_max,
            'New_Min': new_min,
            'New_Max': new_max,
        })
    
    return pd.DataFrame(coverage_list)


def correlation_analysis(old_data, new_data, param_names):
    """ç›¸å…³æ€§åˆ†æ"""
    old_corr = np.corrcoef(old_data.T)
    new_corr = np.corrcoef(new_data.T)
    
    # ç›¸å…³æ€§å·®å¼‚
    corr_diff = new_corr - old_corr
    
    # å¹³å‡ç»å¯¹ç›¸å…³æ€§
    old_mean_corr = np.mean(np.abs(old_corr[np.triu_indices_from(old_corr, k=1)]))
    new_mean_corr = np.mean(np.abs(new_corr[np.triu_indices_from(new_corr, k=1)]))
    
    return old_corr, new_corr, corr_diff, old_mean_corr, new_mean_corr


def pca_analysis(old_data, new_data, param_names, n_components=None):
    """ä¸»æˆåˆ†åˆ†æ"""
    if n_components is None:
        n_components = min(len(param_names), 5)
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    old_scaled = scaler.fit_transform(old_data)
    new_scaled = scaler.transform(new_data)
    
    # PCA
    pca = PCA(n_components=n_components)
    old_pca = pca.fit_transform(old_scaled)
    new_pca = pca.transform(new_scaled)
    
    # è§£é‡Šæ–¹å·®
    explained_var = pca.explained_variance_ratio_
    cumulative_var = np.cumsum(explained_var)
    
    # è½½è·çŸ©é˜µ
    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
    
    return old_pca, new_pca, explained_var, cumulative_var, loadings, pca


def cluster_analysis(old_data, new_data, n_clusters=5):
    """èšç±»åˆ†æ"""
    scaler = StandardScaler()
    old_scaled = scaler.fit_transform(old_data)
    new_scaled = scaler.transform(new_data)
    
    # K-meansèšç±»
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    old_labels = kmeans.fit_predict(old_scaled)
    new_labels = kmeans.predict(new_scaled)
    
    # è®¡ç®—æ–°æ•°æ®åœ¨å„ç°‡ä¸­çš„åˆ†å¸ƒ
    old_cluster_counts = np.bincount(old_labels, minlength=n_clusters)
    new_cluster_counts = np.bincount(new_labels, minlength=n_clusters)
    
    old_cluster_pct = 100 * old_cluster_counts / len(old_labels)
    new_cluster_pct = 100 * new_cluster_counts / len(new_labels)
    
    return old_labels, new_labels, old_cluster_pct, new_cluster_pct, kmeans


def sample_sufficiency_analysis(old_data, new_data, param_names):
    """æ ·æœ¬å……è¶³æ€§åˆ†æ"""
    n_params = len(param_names)
    old_n = len(old_data)
    new_n = len(new_data)
    
    # ç»éªŒæ³•åˆ™
    min_samples_per_param = 10  # æ¯ä¸ªå‚æ•°è‡³å°‘10ä¸ªæ ·æœ¬
    recommended_min = n_params * min_samples_per_param
    
    # åŸºäºæ–¹å·®ç¨³å®šæ€§çš„æ ·æœ¬é‡ä¼°è®¡
    old_cv = np.std(old_data, axis=0) / (np.mean(old_data, axis=0) + 1e-8)
    new_cv = np.std(new_data, axis=0) / (np.mean(new_data, axis=0) + 1e-8)
    
    # ä¼°è®¡è¾¾åˆ°CV<0.1æ‰€éœ€çš„æ ·æœ¬é‡ï¼ˆç²—ç•¥ä¼°è®¡ï¼‰
    target_cv = 0.1
    estimated_n = []
    for i in range(n_params):
        if new_cv[i] > target_cv:
            # n âˆ 1/CVÂ²
            n_needed = new_n * (new_cv[i] / target_cv)**2
            estimated_n.append(int(n_needed))
        else:
            estimated_n.append(new_n)
    
    max_n_needed = max(estimated_n)
    
    return {
        'old_n': old_n,
        'new_n': new_n,
        'n_params': n_params,
        'recommended_min': recommended_min,
        'estimated_n_for_stability': max_n_needed,
        'old_cv': old_cv,
        'new_cv': new_cv,
        'estimated_n_per_param': estimated_n
    }


# ============ å¯è§†åŒ– ============

def plot_comprehensive_analysis(old_data, new_data, param_names, output_dir):
    """ç”Ÿæˆç»¼åˆåˆ†æå›¾è¡¨"""
    os.makedirs(output_dir, exist_ok=True)
    
    n_params = len(param_names)
    
    # 1. åˆ†å¸ƒå¯¹æ¯”ï¼ˆç›´æ–¹å›¾ + ç®±çº¿å›¾ï¼‰
    fig = plt.figure(figsize=(24, 16))
    
    for i, param in enumerate(param_names):
        # ç›´æ–¹å›¾
        ax1 = plt.subplot(4, n_params, i + 1)
        ax1.hist(old_data[:, i], bins=30, alpha=0.6, label='Old', color='blue', density=True)
        ax1.hist(new_data[:, i], bins=30, alpha=0.6, label='New', color='red', density=True)
        ax1.set_title(param, fontsize=10, fontweight='bold')
        ax1.set_xlabel('Value', fontsize=8)
        ax1.set_ylabel('Density', fontsize=8)
        ax1.legend(fontsize=8)
        ax1.grid(True, alpha=0.3)
        
        # ç®±çº¿å›¾
        ax2 = plt.subplot(4, n_params, n_params + i + 1)
        bp = ax2.boxplot([old_data[:, i], new_data[:, i]], 
                          labels=['Old', 'New'], patch_artist=True)
        bp['boxes'][0].set_facecolor('lightblue')
        bp['boxes'][1].set_facecolor('lightcoral')
        ax2.set_title(param, fontsize=10)
        ax2.set_ylabel('Value', fontsize=8)
        ax2.grid(True, alpha=0.3, axis='y')
        
        # Q-Qå›¾ï¼ˆæ­£æ€æ€§æ£€éªŒï¼‰
        ax3 = plt.subplot(4, n_params, 2*n_params + i + 1)
        stats.probplot(new_data[:, i], dist="norm", plot=ax3)
        ax3.set_title(f'{param} Q-Q', fontsize=10)
        ax3.grid(True, alpha=0.3)
        
        # æ ¸å¯†åº¦ä¼°è®¡
        ax4 = plt.subplot(4, n_params, 3*n_params + i + 1)
        from scipy.stats import gaussian_kde
        if len(np.unique(old_data[:, i])) > 1:
            kde_old = gaussian_kde(old_data[:, i])
            x_old = np.linspace(old_data[:, i].min(), old_data[:, i].max(), 100)
            ax4.plot(x_old, kde_old(x_old), label='Old', color='blue', linewidth=2)
        if len(np.unique(new_data[:, i])) > 1:
            kde_new = gaussian_kde(new_data[:, i])
            x_new = np.linspace(new_data[:, i].min(), new_data[:, i].max(), 100)
            ax4.plot(x_new, kde_new(x_new), label='New', color='red', linewidth=2)
        ax4.set_title(f'{param} KDE', fontsize=10)
        ax4.set_xlabel('Value', fontsize=8)
        ax4.set_ylabel('Density', fontsize=8)
        ax4.legend(fontsize=8)
        ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, '01_distribution_analysis.png'), dpi=200, bbox_inches='tight')
    plt.close()
    
    # 2. ç›¸å…³æ€§çŸ©é˜µå¯¹æ¯”
    old_corr, new_corr, corr_diff, _, _ = correlation_analysis(old_data, new_data, param_names)
    
    fig, axes = plt.subplots(1, 3, figsize=(20, 6))
    
    im1 = axes[0].imshow(old_corr, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')
    axes[0].set_title('Old Data - Correlation Matrix', fontsize=14, fontweight='bold')
    axes[0].set_xticks(range(len(param_names)))
    axes[0].set_yticks(range(len(param_names)))
    axes[0].set_xticklabels(param_names, rotation=45, ha='right', fontsize=10)
    axes[0].set_yticklabels(param_names, fontsize=10)
    for i in range(len(param_names)):
        for j in range(len(param_names)):
            axes[0].text(j, i, f'{old_corr[i,j]:.2f}', ha='center', va='center',
                        color='white' if abs(old_corr[i,j]) > 0.5 else 'black', fontsize=8)
    plt.colorbar(im1, ax=axes[0])
    
    im2 = axes[1].imshow(new_corr, cmap='coolwarm', vmin=-1, vmax=1, aspect='auto')
    axes[1].set_title('New Data - Correlation Matrix', fontsize=14, fontweight='bold')
    axes[1].set_xticks(range(len(param_names)))
    axes[1].set_yticks(range(len(param_names)))
    axes[1].set_xticklabels(param_names, rotation=45, ha='right', fontsize=10)
    axes[1].set_yticklabels(param_names, fontsize=10)
    for i in range(len(param_names)):
        for j in range(len(param_names)):
            axes[1].text(j, i, f'{new_corr[i,j]:.2f}', ha='center', va='center',
                        color='white' if abs(new_corr[i,j]) > 0.5 else 'black', fontsize=8)
    plt.colorbar(im2, ax=axes[1])
    
    im3 = axes[2].imshow(corr_diff, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')
    axes[2].set_title('Correlation Difference (New - Old)', fontsize=14, fontweight='bold')
    axes[2].set_xticks(range(len(param_names)))
    axes[2].set_yticks(range(len(param_names)))
    axes[2].set_xticklabels(param_names, rotation=45, ha='right', fontsize=10)
    axes[2].set_yticklabels(param_names, fontsize=10)
    for i in range(len(param_names)):
        for j in range(len(param_names)):
            axes[2].text(j, i, f'{corr_diff[i,j]:+.2f}', ha='center', va='center',
                        color='white' if abs(corr_diff[i,j]) > 0.3 else 'black', fontsize=8)
    plt.colorbar(im3, ax=axes[2])
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, '02_correlation_analysis.png'), dpi=200, bbox_inches='tight')
    plt.close()
    
    # 3. PCAåˆ†æ
    old_pca, new_pca, explained_var, cumulative_var, loadings, pca_model = pca_analysis(
        old_data, new_data, param_names, n_components=min(len(param_names), 5)
    )
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # PC1 vs PC2 æ•£ç‚¹å›¾
    axes[0, 0].scatter(old_pca[:, 0], old_pca[:, 1], alpha=0.5, s=20, label='Old', color='blue')
    axes[0, 0].scatter(new_pca[:, 0], new_pca[:, 1], alpha=0.8, s=50, label='New', color='red', marker='^')
    axes[0, 0].set_xlabel('PC1', fontsize=12)
    axes[0, 0].set_ylabel('PC2', fontsize=12)
    axes[0, 0].set_title('PCA: PC1 vs PC2', fontsize=14, fontweight='bold')
    axes[0, 0].legend(fontsize=10)
    axes[0, 0].grid(True, alpha=0.3)
    
    # è§£é‡Šæ–¹å·®
    axes[0, 1].bar(range(1, len(explained_var)+1), explained_var, alpha=0.7, color='steelblue')
    axes[0, 1].plot(range(1, len(cumulative_var)+1), cumulative_var, 'ro-', linewidth=2, markersize=8)
    axes[0, 1].set_xlabel('Principal Component', fontsize=12)
    axes[0, 1].set_ylabel('Explained Variance Ratio', fontsize=12)
    axes[0, 1].set_title('Scree Plot', fontsize=14, fontweight='bold')
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].legend(['Cumulative', 'Individual'], fontsize=10)
    
    # è½½è·çŸ©é˜µçƒ­åŠ›å›¾
    im = axes[0, 2].imshow(loadings[:, :min(3, loadings.shape[1])], cmap='RdBu_r', aspect='auto')
    axes[0, 2].set_title('PCA Loadings', fontsize=14, fontweight='bold')
    axes[0, 2].set_xlabel('Principal Component', fontsize=12)
    axes[0, 2].set_ylabel('Original Feature', fontsize=12)
    axes[0, 2].set_yticks(range(len(param_names)))
    axes[0, 2].set_yticklabels(param_names, fontsize=10)
    axes[0, 2].set_xticks(range(min(3, loadings.shape[1])))
    axes[0, 2].set_xticklabels([f'PC{i+1}' for i in range(min(3, loadings.shape[1]))], fontsize=10)
    plt.colorbar(im, ax=axes[0, 2])
    
    # PC1 vs PC3
    if old_pca.shape[1] > 2:
        axes[1, 0].scatter(old_pca[:, 0], old_pca[:, 2], alpha=0.5, s=20, label='Old', color='blue')
        axes[1, 0].scatter(new_pca[:, 0], new_pca[:, 2], alpha=0.8, s=50, label='New', color='red', marker='^')
        axes[1, 0].set_xlabel('PC1', fontsize=12)
        axes[1, 0].set_ylabel('PC3', fontsize=12)
        axes[1, 0].set_title('PCA: PC1 vs PC3', fontsize=14, fontweight='bold')
        axes[1, 0].legend(fontsize=10)
        axes[1, 0].grid(True, alpha=0.3)
    
    # PC2 vs PC3
    if old_pca.shape[1] > 2:
        axes[1, 1].scatter(old_pca[:, 1], old_pca[:, 2], alpha=0.5, s=20, label='Old', color='blue')
        axes[1, 1].scatter(new_pca[:, 1], new_pca[:, 2], alpha=0.8, s=50, label='New', color='red', marker='^')
        axes[1, 1].set_xlabel('PC2', fontsize=12)
        axes[1, 1].set_ylabel('PC3', fontsize=12)
        axes[1, 1].set_title('PCA: PC2 vs PC3', fontsize=14, fontweight='bold')
        axes[1, 1].legend(fontsize=10)
        axes[1, 1].grid(True, alpha=0.3)
    
    # è½½è·å‘é‡å›¾ï¼ˆbiplotï¼‰
    axes[1, 2].scatter(old_pca[:, 0], old_pca[:, 1], alpha=0.3, s=10, color='lightblue')
    axes[1, 2].scatter(new_pca[:, 0], new_pca[:, 1], alpha=0.6, s=30, color='lightcoral', marker='^')
    
    # ç»˜åˆ¶è½½è·å‘é‡
    scale_factor = 3
    for i, param in enumerate(param_names):
        axes[1, 2].arrow(0, 0, loadings[i, 0]*scale_factor, loadings[i, 1]*scale_factor,
                        head_width=0.1, head_length=0.1, fc='black', ec='black', linewidth=2)
        axes[1, 2].text(loadings[i, 0]*scale_factor*1.15, loadings[i, 1]*scale_factor*1.15,
                       param, fontsize=10, ha='center', va='center',
                       bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
    
    axes[1, 2].set_xlabel('PC1', fontsize=12)
    axes[1, 2].set_ylabel('PC2', fontsize=12)
    axes[1, 2].set_title('PCA Biplot', fontsize=14, fontweight='bold')
    axes[1, 2].grid(True, alpha=0.3)
    axes[1, 2].axhline(y=0, color='k', linestyle='--', linewidth=0.5)
    axes[1, 2].axvline(x=0, color='k', linestyle='--', linewidth=0.5)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, '03_pca_analysis.png'), dpi=200, bbox_inches='tight')
    plt.close()
    
    # 4. èšç±»åˆ†æ
    old_labels, new_labels, old_cluster_pct, new_cluster_pct, kmeans_model = cluster_analysis(
        old_data, new_data, n_clusters=min(5, len(old_data)//10)
    )
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # èšç±»åˆ†å¸ƒå¯¹æ¯”
    x = np.arange(len(old_cluster_pct))
    width = 0.35
    axes[0].bar(x - width/2, old_cluster_pct, width, label='Old', color='blue', alpha=0.7)
    axes[0].bar(x + width/2, new_cluster_pct, width, label='New', color='red', alpha=0.7)
    axes[0].set_xlabel('Cluster ID', fontsize=12)
    axes[0].set_ylabel('Percentage (%)', fontsize=12)
    axes[0].set_title('Cluster Distribution Comparison', fontsize=14, fontweight='bold')
    axes[0].set_xticks(x)
    axes[0].legend(fontsize=10)
    axes[0].grid(True, alpha=0.3, axis='y')
    
    # PCAç©ºé—´ä¸­çš„èšç±»
    scaler = StandardScaler()
    old_scaled = scaler.fit_transform(old_data)
    new_scaled = scaler.transform(new_data)
    pca_viz = PCA(n_components=2)
    old_pca_viz = pca_viz.fit_transform(old_scaled)
    new_pca_viz = pca_viz.transform(new_scaled)
    
    for cluster_id in range(len(old_cluster_pct)):
        mask_old = old_labels == cluster_id
        axes[1].scatter(old_pca_viz[mask_old, 0], old_pca_viz[mask_old, 1],
                       alpha=0.5, s=20, label=f'Old C{cluster_id}')
    
    for cluster_id in range(len(new_cluster_pct)):
        mask_new = new_labels == cluster_id
        if np.sum(mask_new) > 0:
            axes[1].scatter(new_pca_viz[mask_new, 0], new_pca_viz[mask_new, 1],
                           alpha=0.8, s=100, marker='^', edgecolors='black', linewidth=1.5,
                           label=f'New C{cluster_id}')
    
    axes[1].set_xlabel('PC1', fontsize=12)
    axes[1].set_ylabel('PC2', fontsize=12)
    axes[1].set_title('Clusters in PCA Space', fontsize=14, fontweight='bold')
    axes[1].legend(fontsize=8, ncol=2)
    axes[1].grid(True, alpha=0.3)
    
    # ç°‡é—´è·ç¦»
    centers = kmeans_model.cluster_centers_
    from scipy.spatial.distance import pdist, squareform
    distances = squareform(pdist(centers, metric='euclidean'))
    
    im = axes[2].imshow(distances, cmap='YlOrRd', aspect='auto')
    axes[2].set_title('Inter-Cluster Distances', fontsize=14, fontweight='bold')
    axes[2].set_xlabel('Cluster ID', fontsize=12)
    axes[2].set_ylabel('Cluster ID', fontsize=12)
    axes[2].set_xticks(range(len(old_cluster_pct)))
    axes[2].set_yticks(range(len(old_cluster_pct)))
    for i in range(len(old_cluster_pct)):
        for j in range(len(old_cluster_pct)):
            axes[2].text(j, i, f'{distances[i,j]:.2f}', ha='center', va='center',
                        color='white' if distances[i,j] > distances.max()/2 else 'black', fontsize=10)
    plt.colorbar(im, ax=axes[2])
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, '04_cluster_analysis.png'), dpi=200, bbox_inches='tight')
    plt.close()
    
    # 5. å‚æ•°ç©ºé—´è¦†ç›–åº¦
    coverage_df = parameter_space_coverage(old_data, new_data, param_names)
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # èŒƒå›´å¯¹æ¯”
    x = np.arange(len(param_names))
    width = 0.35
    axes[0, 0].bar(x - width/2, coverage_df['Old_Range'], width, label='Old', color='blue', alpha=0.7)
    axes[0, 0].bar(x + width/2, coverage_df['New_Range'], width, label='New', color='red', alpha=0.7)
    axes[0, 0].set_xlabel('Parameter', fontsize=12)
    axes[0, 0].set_ylabel('Range', fontsize=12)
    axes[0, 0].set_title('Parameter Range Comparison', fontsize=14, fontweight='bold')
    axes[0, 0].set_xticks(x)
    axes[0, 0].set_xticklabels(param_names, rotation=45, ha='right', fontsize=10)
    axes[0, 0].legend(fontsize=10)
    axes[0, 0].grid(True, alpha=0.3, axis='y')
    
    # è¦†ç›–åº¦ç™¾åˆ†æ¯”
    axes[0, 1].bar(x, coverage_df['Range_Coverage_%'], color='steelblue', alpha=0.7)
    axes[0, 1].axhline(y=100, color='red', linestyle='--', linewidth=2, label='100% Coverage')
    axes[0, 1].set_xlabel('Parameter', fontsize=12)
    axes[0, 1].set_ylabel('Coverage (%)', fontsize=12)
    axes[0, 1].set_title('New Data Range Coverage of Old Data', fontsize=14, fontweight='bold')
    axes[0, 1].set_xticks(x)
    axes[0, 1].set_xticklabels(param_names, rotation=45, ha='right', fontsize=10)
    axes[0, 1].legend(fontsize=10)
    axes[0, 1].grid(True, alpha=0.3, axis='y')
    
    # é‡‡æ ·å¯†åº¦å¯¹æ¯”
    axes[1, 0].bar(x - width/2, coverage_df['Old_Density'], width, label='Old', color='blue', alpha=0.7)
    axes[1, 0].bar(x + width/2, coverage_df['New_Density'], width, label='New', color='red', alpha=0.7)
    axes[1, 0].set_xlabel('Parameter', fontsize=12)
    axes[1, 0].set_ylabel('Density (samples/range)', fontsize=12)
    axes[1, 0].set_title('Sampling Density Comparison', fontsize=14, fontweight='bold')
    axes[1, 0].set_xticks(x)
    axes[1, 0].set_xticklabels(param_names, rotation=45, ha='right', fontsize=10)
    axes[1, 0].legend(fontsize=10)
    axes[1, 0].grid(True, alpha=0.3, axis='y')
    axes[1, 0].set_yscale('log')
    
    # èŒƒå›´å¯è§†åŒ–
    for i, param in enumerate(param_names):
        old_min = coverage_df.loc[i, 'Old_Min']
        old_max = coverage_df.loc[i, 'Old_Max']
        new_min = coverage_df.loc[i, 'New_Min']
        new_max = coverage_df.loc[i, 'New_Max']
        
        axes[1, 1].plot([old_min, old_max], [i-0.15, i-0.15], 'b-', linewidth=8, alpha=0.7, label='Old' if i==0 else '')
        axes[1, 1].plot([new_min, new_max], [i+0.15, i+0.15], 'r-', linewidth=8, alpha=0.7, label='New' if i==0 else '')
        
        # æ ‡è®°é‡å åŒºåŸŸ
        overlap_min = max(old_min, new_min)
        overlap_max = min(old_max, new_max)
        if overlap_min < overlap_max:
            axes[1, 1].plot([overlap_min, overlap_max], [i, i], 'g-', linewidth=4, alpha=0.9,
                           label='Overlap' if i==0 else '')
    
    axes[1, 1].set_yticks(range(len(param_names)))
    axes[1, 1].set_yticklabels(param_names, fontsize=10)
    axes[1, 1].set_xlabel('Parameter Value', fontsize=12)
    axes[1, 1].set_title('Parameter Range Overlap', fontsize=14, fontweight='bold')
    axes[1, 1].legend(fontsize=10)
    axes[1, 1].grid(True, alpha=0.3, axis='x')
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, '05_coverage_analysis.png'), dpi=200, bbox_inches='tight')
    plt.close()
    
    print(f"  âœ“ Generated 5 comprehensive analysis plots")


# ============ ä¸»å‡½æ•° ============

def main():
    print("="*70)
    print("ç§‘å­¦æ•°æ®åˆ†æå·¥å…·ï¼šæ–°æ—§æ•°æ®æ·±åº¦å¯¹æ¯”")
    print("="*70)
    
    os.makedirs(Config.output_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    print("\n[1/5] Loading data...")
    old_data, old_names = load_data(Config.old_excel, Config.old_sheet, Config.key_alias)
    new_data, new_names = load_data(Config.new_excel, Config.new_sheet, Config.key_alias)
    
    if old_data is None or new_data is None:
        print("  âœ— Failed to load data")
        return
    
    print(f"  âœ“ Old data: {old_data.shape[0]} samples Ã— {old_data.shape[1]} parameters")
    print(f"  âœ“ New data: {new_data.shape[0]} samples Ã— {new_data.shape[1]} parameters")
    
    # ç¡®ä¿å‚æ•°åä¸€è‡´
    if old_names != new_names:
        common_names = list(set(old_names) & set(new_names))
        print(f"  âš  Warning: Using {len(common_names)} common parameters")
        param_names = common_names
    else:
        param_names = old_names
    
    # ç»Ÿè®¡åˆ†æ
    print("\n[2/5] Performing statistical analysis...")
    stats_df = descriptive_statistics(old_data, new_data, param_names)
    stats_df.to_excel(os.path.join(Config.output_dir, 'statistical_analysis.xlsx'), index=False)
    print("  âœ“ Statistical analysis completed")
    
    # å‚æ•°ç©ºé—´è¦†ç›–åº¦
    print("\n[3/5] Analyzing parameter space coverage...")
    coverage_df = parameter_space_coverage(old_data, new_data, param_names)
    coverage_df.to_excel(os.path.join(Config.output_dir, 'parameter_coverage.xlsx'), index=False)
    print("  âœ“ Coverage analysis completed")
    
    # æ ·æœ¬å……è¶³æ€§
    print("\n[4/5] Assessing sample sufficiency...")
    sufficiency = sample_sufficiency_analysis(old_data, new_data, param_names)
    
    sufficiency_df = pd.DataFrame({
        'Metric': ['Old N', 'New N', 'N Parameters', 'Recommended Min', 'Estimated N for Stability'],
        'Value': [sufficiency['old_n'], sufficiency['new_n'], sufficiency['n_params'],
                 sufficiency['recommended_min'], sufficiency['estimated_n_for_stability']]
    })
    sufficiency_df.to_excel(os.path.join(Config.output_dir, 'sample_sufficiency.xlsx'), index=False)
    print("  âœ“ Sufficiency analysis completed")
    
    # å¯è§†åŒ–
    print("\n[5/5] Generating comprehensive visualizations...")
    plot_comprehensive_analysis(old_data, new_data, param_names, Config.output_dir)
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    print("\n" + "="*70)
    print("ANALYSIS SUMMARY")
    print("="*70)
    
    print(f"\nğŸ“Š Dataset Overview:")
    print(f"  Old dataset: {old_data.shape[0]} samples, {old_data.shape[1]} parameters")
    print(f"  New dataset: {new_data.shape[0]} samples, {new_data.shape[1]} parameters")
    
    print(f"\nğŸ” Statistical Tests:")
    sig_params = stats_df[stats_df['p_value_ks'] < 0.05]['Parameter'].tolist()
    if sig_params:
        print(f"  âš  Significant distribution differences (p<0.05): {', '.join(sig_params)}")
    else:
        print(f"  âœ“ No significant distribution differences detected")
    
    print(f"\nğŸ“ Parameter Space Coverage:")
    low_coverage = coverage_df[coverage_df['Range_Coverage_%'] < 30]['Parameter'].tolist()
    if low_coverage:
        print(f"  ğŸ”´ Low coverage (<30%): {', '.join(low_coverage)}")
    else:
        print(f"  âœ“ Adequate coverage for all parameters")
    
    print(f"\nğŸ“ˆ Sample Sufficiency:")
    print(f"  Current new samples: {sufficiency['new_n']}")
    print(f"  Recommended minimum: {sufficiency['recommended_min']}")
    print(f"  Estimated for stability: {sufficiency['estimated_n_for_stability']}")
    
    if sufficiency['new_n'] < sufficiency['recommended_min']:
        print(f"  ğŸ”´ INSUFFICIENT: Need {sufficiency['recommended_min'] - sufficiency['new_n']} more samples")
    elif sufficiency['new_n'] < sufficiency['estimated_n_for_stability']:
        print(f"  ğŸŸ¡ MARGINAL: {sufficiency['estimated_n_for_stability'] - sufficiency['new_n']} more samples recommended")
    else:
        print(f"  âœ“ SUFFICIENT for current purposes")
    
    print(f"\nğŸ“ Results saved to: {Config.output_dir}")
    print(f"  - statistical_analysis.xlsx (detailed statistics)")
    print(f"  - parameter_coverage.xlsx (coverage metrics)")
    print(f"  - sample_sufficiency.xlsx (sample size assessment)")
    print(f"  - 01_distribution_analysis.png")
    print(f"  - 02_correlation_analysis.png")
    print(f"  - 03_pca_analysis.png")
    print(f"  - 04_cluster_analysis.png")
    print(f"  - 05_coverage_analysis.png")
    
    print("\n" + "="*70)
    print("âœ… Analysis completed successfully!")
    print("="*70)


if __name__ == "__main__":
    main()
